This thesis presented a unified framework for generating interpretable, high-fidelity counterfactual explanations (CEs) for autonomous driving decisions using deep generative models. By leveraging a Variational Autoencoder (VAE) architecture trained on high-dimensional RGB scenes from the CARLA simulator, the proposed system aimed to uncover minimal, semantically meaningful changes that influence model predictions, thereby improving transparency and trust in safety-critical AI applications.

The study was guided by four core research questions, each targeting a distinct component of the explainability pipeline:


\section{Answering RQ1: VAE and Classifier Implementation}

To evaluate whether a VAE can encode and reconstruct semantically rich driving scenes in a compact latent space, designed a deep convolutional encoder-decoder with KL annealing and flexible loss functions. Experimental results showed that the learned 128-dimensional latent representations retained essential driving semantics. These representations supported accurate downstream classification tasks using multiple models (MLP, SVM, RF, KNN, Logistic Regression), with the MLP and SVM classifiers achieving up to 89\% test accuracy on a 4-class problem. PCA and t-SNE projections confirmed moderate latent space clustering despite the VAE being trained in an unsupervised manner. Thus, the VAE effectively captured meaningful structure in high-dimensional input data, validating RQ1.

\section{Answering RQ2: Loss Function Optimization}

To investigate how reconstruction losses affect model performance, two VAE variants were trained using MSE and Log-Cosh losses. Comprehensive quantitative and qualitative analyses (including PSNR, SSIM, reconstruction visuals, and loss convergence) demonstrated that Log-Cosh consistently outperformed MSE across all metrics. The Log-Cosh variant achieved better perceptual fidelity (SSIM = 0.864, PSNR = 31.04 dB), smoother loss convergence, and superior image quality. These findings strongly support the adoption of robust loss functions like Log-Cosh for generative modeling in autonomous systems, thereby addressing RQ2.

\section{Answering RQ3: Comparative Evaluation of Masking-Based Counterfactuals}

Five masking-based CE generation strategies were proposed and evaluated in which three operating in image space (Grid-Based, Object Detection, LIME on Image) and two in latent space (LIME on Latent Features, LIME on Latent Features using NUN). A unified pipeline was developed to ensure methodological consistency across all techniques.

Evaluation across multiple dimensions counterfactual explanations coverage, failure rate, runtime, proximity, sparsity, and method overlap—revealed that:

\begin{itemize}
    \item \textbf{LIME on Latent NUN} was the most effective, achieving high CE coverage (97.44\%), class-wise balance, and realistic counterfactuals, albeit with the highest runtime.
    \item \textbf{Grid-Based Masking} emerged as a strong, fast baseline (coverage = 98.89\%, runtime = 13 min) with excellent interpretability and semantic targeting.
    \item \textbf{LIME on Image} produced visually noisy outputs with low interpretability and high distortion due to aggressive masking of critical regions.
    \item \textbf{LIME on Latent (Median)} generated the sparsest and most localized perturbations (avg. 2.3 features), but had poor validity and coverage.
    \item \textbf{Object Detection Masking} performed poorly due to low detection rates in the dataset.
\end{itemize}

These results demonstrate that counterfactual quality depends not only on prediction change but also on visual fidelity, minimality, and semantic plausibility especially in high-stakes domains like autonomous driving. The LIME on Latent NUN method best balances these trade-offs, thereby answering RQ3.

\section{Answering RQ4: Human-Centered Evaluation of Explanation Preferences}

To evaluate real-world preference and interpretability, a web-based user study was conducted with four masking-based counterfactual methods. Participants rated CEs on three dimensions: interpretability, plausibility, and visual coherence.

Results showed that:

\begin{itemize}
    \item \textbf{Grid-Based Masking} was rated highest in interpretability (4.32) and visual coherence (4.16), praised for structured, minimal changes.
    \item \textbf{LIME on Latent NUN} scored highest in plausibility (3.96) and offered the best balance between realism and decision change.
    \item \textbf{LIME on Image} consistently received the lowest ratings across all dimensions, due to unnatural reconstructions and over-masking.
\end{itemize}

Qualitative user feedback validated the metrics: participants highlighted realism and minimalism as key qualities for understandable counterfactuals. These findings demonstrate the importance of aligning algorithmic explanations with human intuition, thereby answering RQ4.

\section{Thesis Contributions}

The main contributions of this thesis are:
\begin{itemize}
    \item A unified, modular framework for generating and evaluating image-based counterfactuals in autonomous driving.
    \item A comparative study of five masking strategies, with rigorous evaluation using both algorithmic metrics and human preferences.
    \item An effective integration of LIME and NUN in latent space, producing high-quality, semantically grounded counterfactuals.
    \item A web-based tool and human-centered evaluation protocol for future research in explainability.
\end{itemize}


\subsection{From Real-World Motivation to Practical Impact}
This work was motivated by the critical need for transparency in AI-based decision-making in autonomous vehicles explained in introduction (ref ~\ref{Introduction}). Real-world tragedies such as the fatal incident involving an autonomous Uber in Arizona~\autocite{marshall2019uber}, where an object was misclassified as a harmless obstacle highlight the severe consequences of unexplainable model failures. In such high-stakes scenarios, it is not enough for the system to make a prediction, it must also provide insight into why that prediction was made. The proposed framework addresses this gap by enabling counterfactual explanations that expose the minimal, causally important changes in input responsible for altering the model's decision. By reconstructing semantically coherent and visually interpretable explanations, this system offers a path toward greater transparency, accountability, and ultimately safer deployment of AI in autonomous systems.




\section{Limitations} \label{sec:limitations}
Despite its contributions, this thesis has several limitations that merit consideration. First, all experiments were conducted using the CARLA simulator, which although diverse and realistic in structure may not capture the full variability and complexity of real-world urban driving environments. Consequently, generalizability to real-world datasets remains untested. Second, the object detection-based masking method underperformed primarily due to YOLOv5’s limited generalization on CARLA-generated scenes. Its inability to reliably detect key objects like traffic signs or pedestrians often resulted in incomplete or failed counterfactual explanations.

Third, while the VAE succeeded in compressing visual scenes into semantically useful latent vectors, certain classes particularly \texttt{GO} exhibited significant overlap in the latent space. This latent entanglement posed challenges for both classification and counterfactual generation. Additionally, the classifiers used in this study were trained exclusively on VAE-generated latent representations. Incorporating ensemble learning or multimodal inputs (e.g., raw images, or object metadata) could improve overall robustness and interpretability.

Furthermore, evaluation metrics like proximity and sparsity while well-defined in the structured latent space—were less meaningful in image-space interventions. Due to the artifacts introduced during re-encoding of masked images, these metrics may not reliably reflect the true semantic or perceptual closeness between original and counterfactual samples. Another limitation stems from the static thresholding applied in LIME-based methods. Selecting a fixed number of top-$k$ influential features or using dataset-level medians may not generalize across different tasks or datasets, limiting adaptability. 

Finally, the most effective counterfactual method—LIME-guided latent feature masking using Nearest Unlike Neighbor (NUN) was also the most computationally expensive. Its high runtime (over 263 minutes for a full dataset) poses a constraint for real-time or large-scale deployment. These limitations open several avenues for improvement, which are discussed in the following section.

\section{Future Scope} \label{sec:future_scope}

This thesis opens several promising directions for future research in explainable artificial intelligence for autonomous driving systems. First, more advanced generative models such as $\beta$-VAEs, Vector Quantized VAEs (VQ-VAEs), GAN, or diffusion models can be explored to improve latent disentanglement and image reconstruction quality. These models have shown superior performance in preserving semantic structure and could enhance the plausibility and precision of generated counterfactuals. Additionally, replacing LIME with SHAP (SHapley Additive exPlanations) may provide more theoretically sound and stable feature attributions. Unlike LIME, SHAP offers axiomatic guarantees such as local accuracy and consistency, which could address the limitations associated with static thresholding and importance ranking.

A particularly promising avenue is the use of Conditional VAEs (CVAEs), where supervision via class labels during training would help organize the latent space around discrete class manifolds. This would support more targeted manipulations for counterfactual generation. Moreover, further structuring of the latent space can be pursued through architectural refinements such as tuning KL annealing schedules, dropout configurations, or non-linear activations to improve both classification separability and interpretability.

Real-world applicability remains a key frontier. Future work should extend this framework to benchmark datasets such as Cityscapes, KITTI, or nuScenes, and incorporate multimodal sensor inputs like LiDAR and radar. This integration would test the generalizability of the method beyond simulation and towards production grade perception pipelines. Additionally, adaptive feature selection techniques including reinforcement learning, causal inference, or dynamically thresholded LIME/SHAP could lead to more semantically aligned and minimal counterfactuals.

Another major direction involves real-time deployment. Optimizing the counterfactual pipeline for latency and scalability could support interactive debugging and decision support in safety-critical autonomous driving scenarios. Lastly, establishing scalable human evaluation frameworks and open-source benchmarks with labeled counterfactual ground truths would contribute to standardization in this emerging field. Deployment of such explainability modules into perception and decision-making stacks of autonomous vehicles could support transparency, edge-case diagnostics, and regulatory compliance, ensuring that AI-driven systems remain both accountable and trustworthy in complex, real-world environments.



To conclude, this thesis proposed a modular, interpretable, and human-aligned framework for generating counterfactual explanations using deep generative models in autonomous driving. Results validated across quantitative metrics and human studies demonstrated that combining latent feature attribution with VAE-based reconstructions provides a reliable, actionable pathway for trustworthy AI explanations.

As autonomous vehicles move toward real-world deployment, such counterfactual frameworks especially, when paired with open datasets and user-centric evaluations will be crucial in establishing transparency, accountability, and public trust in machine decision-making.