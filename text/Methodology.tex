This chapter describes the methodology used to generate counterfactual explanations using Variational Autoencoders (VAEs) and evaluate different feature masking strategies to enhance interpretability in autonomous driving systems. The methodology consists of multiple stages, including dataset collection, VAE training, classifier training, feature masking, counterfactual explanation generation, and evaluation.

A high-level workflow of the methodology is shown in Figure . The dataset is collected from the CARLA simulator, preprocessed, and used to train a Variational Autoencoder (VAE) for latent space representation. A classifier is trained to distinguish between "STOP", "GO", "RIGHT, and "LEFT" decisions based on input images. Counterfactual explanations are generated by applying different feature masking techniques to alter the image or latent space representation. Finally, the counterfactuals are evaluated using both AI-based quantitative metrics and human evaluation studies.

\section{Dataset Collection, Labeling, and Splitting Process}
The research involved a comprehensive process of collecting, labeling, and splitting the dataset to prepare it for training and evaluation of the models. The process can be broken down into the following stages:

\subsection{Dataset Collection}
The dataset was collected using the CARLA simulator, an advanced open-source platform designed for autonomous driving research. The simulated environment provided a controlled yet realistic setting for capturing diverse driving scenarios. For this study, Town03 and Town07 were selected as the operational environments, ensuring a mix of structured urban roads and more complex, dynamic settings.

A vehicle model Audi A2 was deployed in autopilot mode to autonomously navigate through these environments while capturing RGB images and corresponding control signals. A front-mounted RGB camera sensor was used to record driving scenes with a 125° field of view (FoV) at a resolution of 160×80 pixels. This resolution was selected to maintain a balance between computational efficiency and the retention of essential visual features for learning-based tasks.

In total, approximately 12,000 images were collected under diverse driving conditions, each paired with corresponding control parameters to facilitate supervised learning tasks. The recorded control signals included the steering angle, which ranged from -1 to 1, where -1 represented full left steering, 0 indicated a straight trajectory, and 1 signified full right steering. The throttle value varied between 0 and 1, with 0 denoting no acceleration and 1 indicating maximum throttle input. Similarly, the brake value ranged from 0 to 1, where 0 represented no braking action, and 1 signified full braking force. These control variables, in combination with the visual data, provided a comprehensive representation of the vehicle's navigation behaviour in the simulated environment.

To ensure robust data acquisition, the collection process incorporated multiple verification mechanisms. Real-time logging was employed to track the number of frames successfully saved, skipped, or lost. Additionally, synchronization between the image files and control data was maintained to avoid inconsistencies. The collected dataset serves as the foundation for both 2-class (STOP vs. GO) and 4-class (STOP, GO, LEFT, RIGHT) classification tasks, as detailed in the subsequent labeling process.

\subsection{Dataset Labelling}

For labeling, we processed the collected data to categorize each instance based on the vehicle's control inputs. We used two labeling schemes: a 2-class scheme distinguishing between STOP and GO, and a 4-class scheme further differentiating between STOP, GO, RIGHT, and LEFT. The labeling was based on thresholds for brake, throttle, and steering values. For the 2-class scheme, instances were labeled as STOP if the brake value exceeded a certain threshold, and GO otherwise. In the 4-class scheme, STOP was prioritized, followed by RIGHT and LEFT if the steering value exceeded specific thresholds and the throttle was above a minimal value, with the remainder labeled as GO. This approach ensured a clear classification of driving actions for both binary and multi-class analysis.

\subsection{Dataset Labeling}
After collecting the dataset, it was labeled to prepare it for training and evaluation. The labeling process involved categorizing each instance based on the vehicle's control inputs using the following mathematical criteria:

\textbf{2-Class Labeling Scheme:}
\begin{itemize}
    \item \textbf{STOP}: 
    \begin{equation}
    \text{label} = \text{STOP} \quad \text{if} \quad \text{brake} > \theta_{\text{STOP}}
    \end{equation}
    \item \textbf{GO}: 
    \begin{equation}
    \text{label} = \text{GO} \quad \text{otherwise}
    \end{equation}
\end{itemize}
where $\theta_{\text{STOP}}$ is the threshold for the brake value.

\textbf{4-Class Labeling Scheme:}
\begin{itemize}
    \item \textbf{STOP}: 
    \begin{equation}
    \text{label} = \text{STOP} \quad \text{if} \quad \text{brake} > \theta_{\text{STOP}}
    \end{equation}
    \item \textbf{RIGHT}: 
    \begin{equation}
    \text{label} = \text{RIGHT} \quad \text{if} \quad \text{steering} > \theta_{\text{TURN}} \quad \text{and} \quad \text{throttle} > \epsilon
    \end{equation}
    \item \textbf{LEFT}: 
    \begin{equation}
    \text{label} = \text{LEFT} \quad \text{if} \quad \text{steering} < -\theta_{\text{TURN}} \quad \text{and} \quad \text{throttle} > \epsilon
    \end{equation}
    \item \textbf{GO}: 
    \begin{equation}
    \text{label} = \text{GO} \quad \text{otherwise}
    \end{equation}
\end{itemize}
where $\theta_{\text{STOP}}$ is the threshold for the brake value, $\theta_{\text{TURN}}$ is the threshold for the steering value, and $\epsilon$ is a minimal throttle value (typically set to 0.1).

These criteria were used to ensure a clear classification of driving actions for both binary and multi-class analysis. The thresholds were determined using either a quantile method, where thresholds were dynamically set based on quantiles of the control data, or a fixed method, where thresholds were manually set.

\section{Variational Autoencoder (VAE)} \label{sec:vae}

To generate plausible and semantically meaningful counterfactual explanations, we employ a Variational Autoencoder (VAE) as the backbone of our approach. This design is inspired by the Contrastive Explanation Method (CEM) \cite{DBLP:journals/corr/abs-1802-07623}, which uses autoencoders to ensure explanations remain on the data manifold, thus appearing realistic to human observers. While CEM focuses on identifying pertinent positives and negatives for simpler datasets like MNIST, our approach extends this principle to the more complex image data of autonomous driving scenarios, incorporating structured latent space manipulations and masking strategies. The VAE encodes images into a continuous latent space, enabling targeted feature modifications that are then decoded into visually coherent counterfactual instances.  Further discussion on the rationale and alignment with CEM can be found in Section~\ref{sec:related_work}.

\subsection{VAE Architecture}  \label{sec:vae_architecture}

% The VAE utilizes a CNN-based encoder-decoder architecture, detailed below and visualized in Figure~\ref{fig:vae_architecture}.

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\textwidth]{vae_architecture.png} % Replace with your actual figure file
%     \caption{Architecture of the Variational Autoencoder (VAE). The encoder (top) compresses the input image into a latent representation (µ, log σ²). The decoder (bottom) reconstructs the image from the sampled latent vector z.}
%     \label{fig:vae_architecture}
% \end{figure}


\subsubsection{Encoder} \label{subsec:encoder}
The encoder comprises four convolutional layers that progressively downsample the input image (160x80 RGB) while increasing the channel depth (3$\to$64$\to$128$\to$256$\to$512).  Each convolutional layer, except the third, is followed by Batch Normalization and a LeakyReLU activation function. The output of the final convolutional layer is flattened and passed through a fully connected layer with 1024 units and LeakyReLU activation. This layer then branches into two separate linear layers, producing the mean ($\mu$) and log-variance ($\log \sigma^2$) of the latent distribution, which parameterize a Gaussian distribution over the 128-dimensional latent space.


\subsubsection{Decoder} \label{subsec:decoder}
Mirroring the encoder, the decoder starts with two fully connected layers that expand the sampled latent vector. These are followed by four transposed convolutional layers that progressively upsample the representation back to the original image dimensions.  LeakyReLU activations follow each transposed convolutional layer except the last, which uses a Sigmoid activation function to ensure output pixel values are within the range [0, 1].


\subsection{VAE Training and Loss Function} \label{sec:vae_training}

The VAE was trained using the Adam optimizer with a learning rate of $1e-4$ and weight decay of $1e-5$. A learning rate scheduler, reducing the learning rate by a factor of 0.5 after 10 epochs of plateauing validation loss, was employed. The batch size was 128. Training proceeded for a maximum of 200 epochs, with early stopping (patience of 50 epochs) based on the validation loss. The KL divergence weight, balancing reconstruction accuracy and latent space regularization, was linearly annealed from an initial value of $5e-5$ by $0.0001$ per epoch, capped at 1.0.  The training process and resulting performance are further discussed in Section~\ref{sec:vae_performance}. % Cross-reference to results section

Two reconstruction loss functions were investigated: Mean Squared Error (MSE) and Log-Cosh loss~\cite{chen2019log}. The Log-Cosh loss, defined as:

$$L_{recon} = (1/a) \cdot \log(\cosh(a \cdot (x - \text{recon}_x)))$$

where $a$ is a scaling parameter set to 1.0, offers improved robustness to outliers compared to MSE while preserving fine image details.  Our experiments, detailed in Section~\ref{sec:vae_performance}, demonstrated that the Log-Cosh loss resulted in [describe the key improvements, e.g., lower reconstruction loss, better visual quality], leading to its selection for the final VAE model. The reparameterization trick was employed during training to enable backpropagation through the stochastic sampling of the latent variables.

\subsection{Evaluating VAE Performance}


\section{Classifier Model for Prediction}

\subsection{Model Selection}
Type of classifier used: CNN, Random forest, SVM etc.

\subsection{Training Setup}
Loss function, optimizer, training/testing dataset split

\subsection{Classifier Performance Evaluation}
Accuracy, precision, recall, F1-score, confusion matrix


\section{Feature Masking Techniques for Counterfactual Generation}
How different masking techniques are used to modify input images and Latent features

\subsection{ Grid-Based Masking}

\subsection{Object Detection-Based Masking (YOLOv5)}

\subsection{LIME on Images}

\subsection{LIME-Based Masking on Latent Features}




\section{Generating Counterfactual Explanations}
Process of modifying inputs to generate counterfactuals.

\subsection{Counterfactual Explanation Generation Pipeline}
Step-by-step process of generating counterfactuals.


\subsection{Example of Counterfactual Generation}
a visual example of an image before and after counterfactual modification.