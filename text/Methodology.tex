This chapter describes the methodology used to generate counterfactual explanations using Variational Autoencoders (VAEs) and evaluate different feature masking strategies to enhance interpretability in autonomous driving systems. The methodology consists of multiple stages, including dataset collection, VAE training, classifier training, feature masking, counterfactual explanation generation, and evaluation.

A high-level workflow of the methodology is shown in Figure (let im edraw the methodology diagram and place an image). The dataset is collected from the CARLA simulator, preprocessed, and used to train a Variational Autoencoder (VAE) for latent space representation. A classifier is trained to distinguish between "STOP", "GO", "RIGHT, and "LEFT" decisions based on input images. Counterfactual explanations are generated by applying different feature masking techniques to alter the image or latent space representation. Finally, the counterfactuals are evaluated using both AI-based quantitative metrics and human evaluation studies.

\section{Experimental Setup}

All experiments in this thesis were conducted in a Linux environment to ensure compatibility with the CARLA simulator and associated tools. The setup was built using \textbf{CARLA version 0.9.15}, an open-source urban driving simulator widely adopted for autonomous driving research. This version provides a flexible and high-fidelity simulation environment, making it suitable for collecting diverse, labeled driving data under various urban conditions.

To maintain compatibility with CARLA’s Python API, \textbf{Python version 3.7} was used for dataset collection. This version is recommended by CARLA’s developers to avoid API and dependency conflicts. The dataset was collected using two CARLA maps: \textbf{Town06} and \textbf{Town07} \cite{CARLA2024}. These maps were chosen due to their varied road layouts and environmental features, which provide rich scenarios for evaluating counterfactual explanations. Users replicating this setup should download the CARLA server (v0.9.15) along with the additional maps from the \textit{official CARLA repository}. Once downloaded, the maps must be copied into the main CARLA directory to ensure proper integration.

Before executing any client-side scripts for dataset collection or agent control, it is essential to start the CARLA server using the following command in the CARLA root directory:

\begin{verbatim}
./CarlaUE4.sh
\end{verbatim}

This launches the CARLA simulation in the Unreal Engine environment. For smooth operation, it is recommended to run the project on a system with at least \textbf{16 GB of RAM}, a dedicated GPU (e.g., \textbf{NVIDIA RTX series}), and \textbf{Ubuntu 18.04 or 20.04 LTS}. 

For the development and training of the Variational Autoencoder (VAE) and classifier models, we used \textbf{Python version 3.11 or higher}. This was necessary to avoid compatibility issues with newer versions of PyTorch, which are not well supported on older Python versions like 3.7. While dataset collection was handled using Python 3.7, the core machine learning model development required a more modern Python environment.  

To monitor and interpret various aspects of model training—such as loss, accuracy, weight distributions, and layer activations—we used \textbf{TensorBoard}. It provided valuable insights into training behavior and helped fine-tune model performance.

\section{Dataset Collection, Labeling, and Splitting Process}

To train and evaluate the proposed models effectively, a high-quality and well-structured dataset was essential. This study employed a systematic data preparation pipeline consisting of three key stages: \textit{collection}, \textit{labeling}, and \textit{splitting}. The dataset was first collected using the CARLA simulator, which offers a controllable and realistic environment for simulating diverse driving scenarios. Following data collection, a precise rule-based labeling strategy was applied using vehicle control signals to assign meaningful class labels. Finally, the labeled dataset was partitioned into training and testing subsets to ensure class balance and prevent data leakage. This structured pipeline ensured consistency, reproducibility, and integrity throughout the experimental workflow.

\subsection{Dataset Collection}

The dataset was collected using the CARLA simulator\cite{CARLA2024docs}, an advanced open-source platform for autonomous driving research. This simulation environment allowed for controlled, repeatable, and diverse driving conditions. For this study, Town03 and Town07 were selected as the operational environments to capture a variety of structured urban roads and dynamic driving contexts.

An Audi A2 vehicle was deployed in autopilot mode to autonomously navigate the environment while capturing RGB images and corresponding control signals. A front-mounted RGB camera was configured with a 125° field of view (FoV) and a resolution of $160 \times 80$ pixels. This resolution was selected to balance computational efficiency with sufficient visual detail for model learning.

A total of approximately 12,000 images were collected under varied driving scenarios. Each image was paired with the vehicle’s control parameters: steering, throttle, and brake. The \textit{steering angle} ranged from $-1$ (full left) to $1$ (full right), while \textit{throttle} and \textit{brake} values ranged from $0$ to $1$, representing the intensity of acceleration and braking, respectively. This multi-modal data captured both visual context and driving behavior.

To ensure robustness and completeness, the data collection process incorporated multiple verification mechanisms. Real-time logging tracked the number of frames successfully captured, skipped, or lost. Synchronization between image files and control data was rigorously maintained to prevent inconsistencies. The resulting dataset served as the foundation for both binary (STOP vs. GO) and multi-class (STOP, GO, LEFT, RIGHT) classification tasks.

\subsection{Dataset Labeling}

The collected data was labeled using a deterministic rule-based strategy derived from the vehicle’s control inputs. Two distinct labeling schemes were implemented to support binary and multi-class classification objectives.

For the \textbf{binary-class scheme}, each frame was labeled as either \texttt{STOP} or \texttt{GO}. A frame was labeled as \texttt{STOP} if the brake value exceeded a predefined threshold; otherwise, it was labeled as \texttt{GO}. This distinction effectively captured the vehicle's motion state based on braking behavior.

For the \textbf{multi-class scheme}, labels were assigned based on prioritized control logic:
\begin{itemize}
    \item \texttt{STOP}, if the brake value exceeded a defined threshold;
    \item \texttt{RIGHT}, if the steering value was significantly positive and throttle was active;
    \item \texttt{LEFT}, if the steering value was significantly negative and throttle was active;
    \item \texttt{GO}, for all remaining cases where the vehicle moved straight without braking or significant steering.
\end{itemize}

This approach ensured mutually exclusive and semantically meaningful labels for each image. Threshold values for labeling were empirically determined based on the distribution of control signals across the dataset. The process was fully automated, ensuring reproducibility and eliminating manual labeling bias.

\subsection{Dataset Splitting}

Following labeling, the dataset was divided into separate training and testing subsets. The splitting was performed \textit{after} labeling to maintain label integrity and avoid any form of data leakage. Care was taken to ensure that the distribution of class labels remained balanced across both sets. This was critical for promoting fair learning and evaluation, particularly in the multi-class scenario.

The labeled dataset was partitioned using an 80/20 split ratio, where 80\% of the data was used for training and 20\% for testing. This ensured sufficient data for model learning while preserving a representative set for evaluation.

For the binary classification scheme, the training set contained 4,959 GO and 4,728 STOP samples, while the test set included 1,262 GO and 1,160 STOP samples, maintaining a near-balanced distribution across classes.

Similarly, for the 4-class setting, all classes (GO, STOP, LEFT, RIGHT) were equally represented with 3,327 samples in training and 821 samples per class in testing, as illustrated in Figure~\ref{fig:class_distribution_pie}.

The training set was used exclusively for model optimization, while the test set was held out for final performance evaluation. This separation ensured that the models were assessed on unseen data, allowing for a reliable estimate of generalization capability.



\section{Variational Autoencoder (VAE)} \label{sec:vae}

To generate plausible, realistic counterfactual explanations that remain on the data manifold, we adopt a Variational Autoencoder (VAE) as the generative backbone of our system. This approach draws conceptual motivation from the Contrastive Explanation Method (CEM) \cite{DBLP:journals/corr/abs-1802-07623}, which leverages autoencoders to ensure generated samples are interpretable and lie within the data distribution. Unlike CEM, which was evaluated on low-resolution datasets (e.g., MNIST), our task deals with higher-resolution RGB images of complex driving scenes. Therefore, the VAE architecture, training objective, and evaluation criteria were carefully adapted and tuned over multiple iterations to suit this setting.

\subsection{VAE Architecture} \label{sec:vae_architecture}

The VAE consists of two primary components: a convolutional encoder that transforms the input image into a latent distribution, and a decoder that reconstructs the image from a sampled latent vector. The encoder and decoder are trained jointly using a variational loss function that enforces both reconstruction fidelity and latent space regularization.

\subsubsection{Encoder Architecture} \label{subsubsec:vae_encoder}

The encoder takes an input image of shape $3 \times 80 \times 160$ (RGB, height $\times$ width) and compresses it into a 128-dimensional latent space. The architecture is composed of four convolutional blocks followed by fully connected layers:

\begin{itemize}
    \item \textbf{Conv Layer 1:} 64 filters, kernel size $4 \times 4$, stride 2, no padding. Followed by LeakyReLU.
    \item \textbf{Conv Layer 2:} 128 filters, kernel size $3 \times 3$, stride 2, padding 1. Followed by BatchNorm and LeakyReLU.
    \item \textbf{Conv Layer 3:} 256 filters, kernel size $4 \times 4$, stride 2. Followed by LeakyReLU.
    \item \textbf{Conv Layer 4:} 512 filters, kernel size $3 \times 3$, stride 2. Followed by BatchNorm and LeakyReLU.
\end{itemize}

The final feature map output is flattened and passed through a fully connected layer with 1024 units (with LeakyReLU activation). From this, two separate linear layers output the mean vector $\mu \in \mathbb{R}^{128}$ and log-variance vector $\log\sigma^2 \in \mathbb{R}^{128}$, representing the parameters of the approximate posterior $q(z|x)$.

A latent vector $z$ is sampled using the reparameterization trick:
\[
z = \mu + \sigma \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
\]
This allows the stochastic sampling process to remain differentiable for gradient-based optimization.

\subsubsection{Decoder Architecture} \label{subsubsec:vae_decoder}

The decoder reverses the encoding process and reconstructs an image from the latent vector $z \in \mathbb{R}^{128}$. It consists of two fully connected layers followed by four transposed convolutional layers:

\begin{itemize}
    \item \textbf{Dense Layer 1:} Linear projection from 128 to 1024 units with LeakyReLU.
    \item \textbf{Dense Layer 2:} Linear projection from 1024 to $512 \times 4 \times 9 = 18432$ units, reshaped to $(512, 4, 9)$.
\end{itemize}

The reshaped feature map is then passed through:

\begin{itemize}
    \item \textbf{Deconv Layer 1:} 256 filters, kernel size $4 \times 4$, stride 2, padding 1, output padding (0,1). Followed by LeakyReLU.
    \item \textbf{Deconv Layer 2:} 128 filters, kernel size $4 \times 4$, stride 2, padding 1, output padding (1,1). Followed by LeakyReLU.
    \item \textbf{Deconv Layer 3:} 64 filters, kernel size $4 \times 4$, stride 2. Followed by LeakyReLU.
    \item \textbf{Deconv Layer 4:} 3 filters (RGB), kernel size $4 \times 4$, stride 2. Followed by Sigmoid activation.
\end{itemize}

The final output has shape $3 \times 80 \times 160$, matching the input dimensions. The Sigmoid activation ensures output values lie in the normalized $[0,1]$ range.

\subsubsection{Training Objective and Optimization Strategy} \label{subsubsec:vae_loss}

The training objective for the VAE is the variational loss function:
\[
\mathcal{L}_{\text{VAE}} = \mathcal{L}_{\text{recon}} + \lambda_{\text{KL}} \cdot \mathcal{L}_{\text{KL}}
\]

\paragraph{Reconstruction Loss:} \label{reconstruction_loss}
Two reconstruction losses were implemented and tested:
\begin{itemize}
    \item \textbf{Mean Squared Error (MSE):} Penalizes squared differences between original and reconstructed pixel values. Suitable for pixel-level fidelity.
    \item \textbf{Log-Cosh Loss:} More robust to outliers; behaves like MSE for small errors and like MAE for large errors.
\end{itemize}
The use of Log-Cosh was empirically found to produce smoother reconstructions in early training phases. Which is motivated by Chen et al.~\cite{chen2019log}.

\paragraph{KL Divergence:}
The KL divergence regularizes the latent distribution:
\[
\mathcal{L}_{\text{KL}} = -\frac{1}{2} \sum_{i=1}^{d} \left(1 + \log\sigma_i^2 - \mu_i^2 - \sigma_i^2\right)
\]
It encourages the approximate posterior $q(z|x)$ to stay close to the unit Gaussian prior $p(z) = \mathcal{N}(0, I)$.

\paragraph{KL Weight Annealing:}
To avoid early dominance of the KL term, we implement linear annealing:
\[
\lambda_{\text{KL}} = \min(\lambda_0 + \delta \cdot \text{epoch}, 1.0)
\]
where $\lambda_0 = 5 \times 10^{-5}$ and $\delta = 1 \times 10^{-4}$. This allows the model to prioritize reconstruction initially, and gradually introduce regularization.

\paragraph{Training Configuration:}
\begin{itemize}
    \item Optimizer: Adam
    \item Learning Rate: $1 \times 10^{-4}$; Weight Decay: $1 \times 10^{-5}$
    \item Batch Size: 128
    \item Epochs: 200
    \item Scheduler: \texttt{ReduceLROnPlateau} (patience = 10, factor = 0.5)
    \item Early Stopping: patience = 50
\end{itemize}

Each epoch logs training and validation losses (total, reconstruction, KL), pixel-level accuracy, and PSNR.

\subsection{Evaluating VAE Performance} \label{subsec:vae_evaluation}

The trained VAE is evaluated using both quantitative metrics and qualitative inspection:

\begin{itemize}
    \item \textbf{Reconstruction Loss:} Tracks how well the VAE preserves input features.
    \item \textbf{Pixel Accuracy:} Compares thresholded original and reconstructed images.
    \item \textbf{PSNR:} Measures signal fidelity; higher values imply better quality.
    \item \textbf{Visual Samples:} Reconstructions are saved every 10 epochs to assess structure, texture, and realism.
    \item \textbf{Latent Space Consistency:} Interpolation and prior sampling are used to verify smoothness and semantic continuity of the latent space.
    \item \textbf{Training Curves:} Plotted for all metrics to track convergence and detect overfitting.
\end{itemize}

These evaluations verify that the VAE learns a well-structured latent space and is capable of reconstructing realistic and semantically accurate driving scenes. This learned representation is leveraged in the subsequent stages for generating counterfactual examples, discussed in \autoref{Evaluation and Results}.





\section{Classifier Model for Prediction}

\subsection{Model Selection}
Type of classifier used: CNN, Random forest, SVM etc.

\subsection{Training Setup}
Loss function, optimizer, training/testing dataset split

\subsection{Classifier Performance Evaluation}
Accuracy, precision, recall, F1-score, confusion matrix


\section{Feature Masking Techniques for Counterfactual Generation}
How different masking techniques are used to modify input images and Latent features

\subsection{ Grid-Based Masking}

\subsection{Object Detection-Based Masking (YOLOv5)}

\subsection{LIME on Images}

\subsection{LIME-Based Masking on Latent Features}




\section{Generating Counterfactual Explanations}
Process of modifying inputs to generate counterfactuals.

\subsection{Counterfactual Explanation Generation Pipeline}
Step-by-step process of generating counterfactuals.


\subsection{Example of Counterfactual Generation}
a visual example of an image before and after counterfactual modification.