In this work, we have laid the groundwork for a process of rigorously using suitable masking technique to generate the counterfactual explanation in the driving task. 




Several avenues for future work open up. First, all existing methods make recommendations of how features would need to be altered to receive a desired result, but none of these methods give associated input importance. And second, it would be desirable to formalize the tradeoff between the variational autoencoder capacity and counterfactual faithfulness.




Key findings from AI-based evaluation and human study were compared.
Which masking method performed best overall?
Are AI-based metrics reliable indicators of human preference?
Limitations and future improvements were discussed.