\section{Background and Motivation}
Deep neural networks (DNNs) have demonstrated superior performance in solving complex problems in numerous domains, including image classification~\autocite{s22239544}, medical diagnosis~\autocite{SHAMSHIRBAND2021103627}, natural language processing~\autocite{Wang23}, and autonomous driving~\autocite{s19092064}. However, their inherent complexity often makes them difficult to interpret, leading to concerns about trust and safety.

When humans and machines use complex machine learning (ML) models to make crucial decisions, a vital concern is whether humans can trust the model and its predictions. This requirement is mandatory in many domains such as healthcare (for medical treatment decisions), finance (for automated lending decisions), and autonomous vehicles (for decision-making by self-driving cars) where human lives depend on the decisions made by the ML model. Unfortunately, the decisions of complex models such as DNNs are difficult to explain.

Explainability is the ability to explain the decision-making process of an AI model in terms understandable to the end user~\cite{8631448}. An explainable model provides a clear and intuitive explanation of the decisions made, enabling users to understand why the model produced a particular result. In other words, explainability focuses on why an algorithm made a specific decision and how that decision can be justified.

Recent studies~\cite{Ribeiro2018,chan2022comparativestudyfaithfulnessmetrics,Singh1622975}
 in the domain of explainable ML outline two desired characteristics for explainers: human interpretable i.e., explanations must provide meaningful and qualitative understanding regarding the decision made by the model, by considering human’s limitations, and faithfulness i.e., explanations must correspond to how the model truly behaves. Guided by these desired characteristics, recent research has proposed various methods for explaining ML models. These methods range from feature attribution approaches such as SHAP~\cite{lundberg2017unifiedapproachinterpretingmodel} and LIME~\cite{Ribeiro2018}, example-based approaches such as counterfactual explanations~\cite{wachter2018CE}, and rule-based approaches such as forest-based trees~\cite{SAGI2020124} and DeepRED~\cite{wachter2018CE}.

Deep learning models, for instance, tend to be black boxes of the first kind because they are highly recursive. As the term is presently used in its most common form, an explanation is a separate model that is supposed to replicate most of the behaviour of a black box (for example, ‘the black box says that people who have been delinquent on current credit are more likely to default on a new loan’). Note that the term ‘explanation’ here refers to an understanding of how a model works, as opposed to an explanation of how the world works~\cite{Rudin2019}.

Automated vehicles are promising for decreasing traffic deaths and providing improved mobility, but also pose challenges in addressing the explainability of AI decisions. Autonomous vehicles have to make split-second decisions based on how they classify objects in the scene in front of them. The consequence can be dangerous if a self-driving car suddenly acts abnormally because of a misclassification problem. This is not merely a hypothetical scenario; such incidents are already occurring. For example, a self-driving Uber recently killed a woman in Arizona~\autocite{marshall2019uber}. It was the first known fatality involving a fully autonomous vehicle and a pedestrian. The information reported by anonymous sources claimed that the car software registered an object in front of the vehicle but treated it like a plastic bag or a tumbleweed carried in the wind. This incident underscores the urgent need for explainability in autonomous driving systems, as without a clear understanding of why the model made a particular decision, it is difficult to diagnose errors, improve system reliability, and ensure accountability. Explainable AI methods can provide critical insights into such failures, potentially preventing similar incidents in the future.


The lack of transparency in machine learning models raises significant challenges in domains where explainability is essential for trust and compliance. In safety-critical applications, blindly trusting model predictions is not an option—decisions must be understandable and justifiable. For example, in medical diagnosis, a deep learning model predicting a cancer diagnosis must provide clear reasoning behind its decision to assist doctors in making informed choices. In financial systems, AI models in loan approval processes must explain why an applicant was accepted or rejected to prevent biased decision-making. In security and law enforcement, AI models used for fraud detection or terrorism risk assessment must provide interpretable justifications to ensure fairness and reliability~\autocite{ribeiro2016ML}. An essential criterion for explainable models is that they should bridge the gap between input features and the model’s decision-making process. Ensuring explainability not only enhances trust and user acceptance but also helps in detecting biases, improving system robustness, and meeting regulatory requirements.

Efforts toward explainable AI for autonomous driving have gained momentum, with researchers exploring various interpretability techniques such as saliency maps, attention mechanisms, and rule-based systems~\autocite{bojarski2016endendlearningselfdriving, Ribeiro2018}. However, these approaches often lack actionability. They provide insights into why a decision was made but not how it could have been changed. Counterfactual explanations emerge as a viable approach by identifying the alterations required in input data to yield alternative results. Emerging predominantly in the late 2010s, these explanations aim to provide insights into machine learning decisions by exploring alternative scenarios. They present a human-friendly way to understand complex models \autocite{yeshwanth2023counterfactual}. For example, in the context of autonomous driving, a counterfactual explanation might illustrate how slight adjustments in sensor data, such as images captured by vehicle cameras, could lead to the vehicle stopping rather than proceeding forward. This technique not only offers valuable insights that can improve the model's efficacy and safety but also facilitates a deeper understanding of its decision-making mechanisms. Imagine an autonomous vehicle approaching a pedestrian or dog crossing. The vehicle decides to continue driving, but a counterfactual explanation reveals that if the image data from the camera showed a slightly different configuration of pixels (perhaps indicating the presence of a pedestrian or dog), the model would have decided to stop. This insight could prompt developers to adjust the model to be more sensitive to potential pedestrians, thereby enhancing safety.

This thesis explores counterfactual explanation generation using deep generative models, specifically Variational Autoencoders (VAEs), and compares different feature masking techniques to identify the most effective approach for improving explainability in autonomous driving.



% \section{Importance of Explainability in Machine Learning}

% Explainability is the ability to explain the decision-making process of an AI model in terms understandable to the end user \cite{8631448}. An explainable model provides a clear and intuitive explanation of the decisions made, enabling users to understand why the model produced a particular result. In other words, explainability focuses on why an algorithm made a specific decision and how that decision can be justified.

% Deep learning models, for instance, tend to be black boxes of the first kind because they are highly recursive. As the term is presently used in its most common form, an explanation is a separate model that is supposed to replicate most of the behaviour of a black box (for example, ‘the black box says that people who have been delinquent on current credit are more likely to default on a new loan’). Note that the term ‘explanation’ here refers to an understanding of how a model works, as opposed to an explanation of how the world works \cite{Rudin2019}.   


% Explainability in autonomous driving systems presents significant challenges due to the complexity and critical nature of real-time decision-making. Counterfactual explanations emerge as a viable approach by identifying the alterations required in input data to yield alternative results. Emerging predominantly in the late 2010s, these explanations aim to provide insights into machine learning decisions by exploring alternative scenarios. They present a human-friendly way to understand complex models \cite{yeshwanth2023counterfactual}. For example, in the context of autonomous driving, a counterfactual explanation might illustrate how slight adjustments in sensor data, such as images captured by vehicle cameras, could lead to the vehicle stopping rather than proceeding forward. This technique not only offers valuable insights that can improve the model's efficacy and safety but also facilitates a deeper understanding of its decision-making mechanisms. Imagine an autonomous vehicle approaching a pedestrian or dog crossing. The vehicle decides to continue driving, but a counterfactual explanation reveals that if the image data from the camera showed a slightly different configuration of pixels (perhaps indicating the presence of a pedestrian or dog), the model would have decided to stop. This insight could prompt developers to adjust the model to be more sensitive to potential pedestrians, thereby enhancing safety.


\section{Research Objectives}
The primary goal of this research is to develop a framework for generating counterfactual explanations using deep generative models, particularly Variational Autoencoders (VAEs), and to evaluate different feature masking strategies for improving interpretability in autonomous driving systems. This research aims to enhance the explainability of deep learning models by identifying how minimal changes in input features can alter classification outcomes, making AI-driven decisions more transparent and interpretable. Specifically, this research seeks to:

\begin{itemize}
    \item \textbf{Develop a VAE-based framework} for generating counterfactual explanations in deep learning models used in autonomous driving.
    The framework will leverage Variational Autoencoders (VAEs) to learn a latent space representation of input images, enabling the controlled modification of key features to generate counterfactual explanations. The proposed approach will integrate VAEs with a classifier to analyze changes in model predictions when specific latent features are altered. This process will allow for a systematic investigation of how the latent space can be used to generate human-interpretable counterfactuals in an autonomous driving context.

    \item \textbf{Investigate different feature masking techniques and evaluate their effectiveness in counterfactual generation.}  
    This research will explore multiple feature masking strategies to modify/mask input features and analyze their impact on counterfactual explanations. The methods investigated are detailed in the \cref{Methodology} section. The effectiveness of these feature masking techniques will be examined in terms of their ability to produce meaningful and realistic counterfactuals that lead to a change in model prediction.
    
    \item \textbf{Evaluate the counterfactual explanation framework using quantitative metrics.}  
    The generated counterfactuals will be assessed using well-established \textbf{quantitative evaluation metrics} to ensure that they are realistic, structurally similar to the original images, and capable of altering model predictions. These evaluations are clearly explained in \cref{Evaluation and Results}.

    \item \textbf{Conduct a human evaluation study to assess user preferences and interpretability of counterfactual explanations.}  
    Since counterfactual explanations are intended for human understanding, this research will conduct a user study to evaluate the effectiveness of different counterfactual generation techniques from a human perspective. Participants will be shown counterfactual explanations generated by different masking methods and will be asked to select the most preferred counterfactual explanation among different generated alternatives for the same image. They will then be asked to rate each counterfactual explanation based on the common counterfactual metrics like Interpretability, Plausibility, and visual Coherence. The results from this human evaluation will be compared with AI-based quantitative evaluation metrics to determine whether computational assessment aligns with human judgment.

\end{itemize}


\section{Research Questions}
To achieve the above objectives, this thesis seeks to answer the following key research questions:

\begin{itemize}
    \item \textbf{RQ1:} How can deep generative models effectively encode high-dimensional driving environment data to generate counterfactual explanations that improve autonomous driving interpretability?

    \item \textbf{RQ2:} How can loss function modifications in Variational Autoencoders (VAEs) optimize image reconstruction quality in autonomous driving tasks? 
    
    \item \textbf{RQ3:} How do different masking techniques impact the effectiveness and efficiency of counterfactual explanation generation, in terms of coverage, computational cost, method overlap, and failure rate?  (See \cref{Methodology} for details on the techniques investigated.) 

    \item \textbf{RQ4:} Which counterfactual explanation method is most preferred by users when selecting among generated explanations of the same original image? What factors (interpretability, plausibility, and visual coherence) influence user preference? (See \cref{Methodology} for details on the methods compared.)


\end{itemize}

\section{Thesis Structure (Outline of Chapters)}
This thesis is organized into multiple chapters, each addressing a key aspect of the research. \autoref{Introduction} introduces the motivation behind this work, outlines the problem statement, research objectives and questions, and provides an overview of the thesis structure. The
\autoref{Background} covers foundational concepts. It also introduces relevant evaluation metrics and properties and also applications.
The \autoref{Methodology} presents the core methodological contributions of the thesis. It details the dataset collection, the architecture and training process of the VAE and classifier, various feature masking techniques, and the process for generating counterfactual explanations. The \autoref{Evaluation and Results} reports both AI-based and human-centered evaluations. It presents different metrics and results, followed by an in depth user study and concludes with the overall discussion. The \autoref{Related work} chapter situates this thesis within the context of existing literature by reviewing and comparing prior work by other researchers. It highlights the key contributions of previous studies, identifies research gaps, and explains how this thesis builds upon and extends those works. Furthermore, it discusses how inspiration was drawn from existing methods and how this research addresses the identified limitations. Finally, \autoref{Conclusion and Future Work} summarizes the key contributions, discusses limitations, and outlines potential directions for future research.