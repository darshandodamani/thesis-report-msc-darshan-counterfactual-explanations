As introduced in \cref{Introduction}, this thesis explores the generation of counterfactual explanations (CEs) in image-based autonomous driving scenarios using deep generative models and post-hoc interpretability techniques. The corresponding implementations are detailed in \cref{Methodology}.

This chapter presents the evaluation strategy and experimental findings, structured according to the central research questions (RQs). For each RQ, the evaluation criteria are outlined and later expanded in dedicated result sections.

\textbf{RQ1:} \textit{How can deep generative models be implemented to effectively encode and reconstruct high-dimensional RGB input images of driving scenes for compact and interpretable representation learning in autonomous driving systems?}

\vspace{-1em}

\paragraph{Evaluation:}The VAE is evaluated on two primary fronts: (i) the fidelity of reconstructed images, and (ii) the semantic usefulness of the latent representations for downstream classification. To this end, multiple classifiers—including MLP, SVM, KNN, Logistic Regression, and Random Forest—are trained on the latent vectors extracted by the encoder. Classification performance is assessed for both binary (GO vs. STOP) and multi-class (GO, STOP, LEFT, RIGHT) tasks. Evaluation metrics include reconstruction loss, SSIM, PSNR, classification accuracy, precision, recall, and F1-score.
    
\vspace{1em}

\textbf{RQ2:} \textit{How can loss function modifications in Variational Autoencoders (VAEs) optimize image reconstruction quality in autonomous driving tasks?} 

\vspace{-1em}

\paragraph{Evaluation:}Compare the image quality Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR) and classification metrics (accuracy, F1) between a standard VAE and one using modified loss functions (logcosh loss). This will include both quantitative analysis and qualitative visualizations of reconstructed images.

\vspace{1em}
    
\textbf{RQ3:} \textit{How do different masking techniques impact the effectiveness and efficiency of counterfactual explanation generation, in terms of coverage, computational cost, method overlap, and failure rate?}

\vspace{-1em}



\paragraph{Evaluation:} To compare the five counterfactual generation methods, we use the following evaluation dimensions:
    \begin{itemize} 
        \item \textbf{Coverage:} The percentage of input samples for which a valid counterfactual explanation is found. A valid CE is one that causes a change in the classifier's prediction after masking and reconstruction.
        
        \item \textbf{Failure Rate:} The proportion of inputs for which the method fails to generate a counterfactual. This includes cases where the masked reconstruction does not lead to a prediction flip.
    
        \item \textbf{Computational Cost:} The average time taken (in seconds) to generate a counterfactual for each input. This reflects the practical usability and scalability of the masking strategy.
    
        \item \textbf{Method Overlap:} The number and percentage of cases where multiple methods yield the same counterfactual outcome (i.e., same final predicted label after masking). This metric helps evaluate the redundancy or uniqueness of each technique.

    \end{itemize}

    All masking methods are applied to the same dataset split, using identical model checkpoints for the VAE and classifier. This ensures fair and consistent comparisons across metrics.

\vspace{1em}

\textbf{RQ4:} \textit{ Which counterfactual explanation method is preferred by users when selecting among generated explanations of the same original image? What factors influence user preference? (See \cref{Methodology} for details on the methods compared.)}

\vspace{-1em}

\paragraph{Evaluation:} Evaluation is based on aggregated user responses:
    \begin{itemize}
        \item \textbf{Preference Distribution:} Percentage of total selections made per CE method.
        \item \textbf{Attribute Ratings:} Average scores for interpretability, plausibility, and coherence across methods.
        \item \textbf{Qualitative Feedback:} Comments collected through optional text fields to gain insights into user reasoning.
    \end{itemize}

This dual analysis enables the identification of methods that are not only algorithmically effective but also intuitively meaningful to end users critical for real-world deployment of interpretable AI in autonomous systems.
    


Each of the following sections restates the respective RQ, evaluates the implemented methods against that question, and presents results along with a discussion of key findings.


\section{Evaluation of Variational Autoencoder (RQ1, RQ2)} \label{sec:vae_evaluation}

The Variational Autoencoder (VAE) forms the backbone of this thesis. It serves dual purposes: (i) as a deep generative model that enables the reconstruction and manipulation of high-dimensional driving scene images, and (ii) as a representation learning tool to generate low-dimensional, semantically meaningful latent vectors suitable for downstream driving action classification. The quality of both image reconstruction and latent encoding is crucial for the success of counterfactual generation. Therefore, evaluating the VAE both quantitatively and qualitatively is a vital first step.

To ensure robustness, various architectural configurations and hyperparameter settings were explored, including latent dimensionality, convolutional depth, and KL divergence annealing schedules. One major focus was the choice of reconstruction loss function, which has a significant effect on the quality of output and the stability of training. Two variants were trained. One using traditional Mean Squared Error (MSE) and the other using Log-Cosh, a smoother, outlier-robust alternative motivated by prior work by Chen et al.,~\cite{chen2019log}. This section presents the training dynamics, comparative results between the two losses, and the final analysis of latent space quality.

\subsubsection{Latent Space Selection and Architectural Rationale} \label{sec:latent-space-selection}

An empirical comparison of latent dimensionalities 64, and 128 was conducted to determine the optimal balance between reconstruction fidelity, training stability, and latent representation capacity. All other architectural and training parameters were held constant for a fair comparison.
\begin{itemize}
    \item \textbf{64-Dimensional:} Produced significantly blurry reconstructions refer figure~\ref{fig:vae64_recon}, particularly around edges and textures. While training remained stable, the compressed latent space struggled to retain sufficient visual detail.
    
    \item \textbf{128-Dimensional:} Achieved the best balance — sharp reconstructions refer figure~\ref{fig:vae128_recon}, stable training, and latent vectors that, when visualized using PCA and t-SNE, showed class-specific clustering post hoc see figure~\ref{fig:latent_space_visualizations}.
\end{itemize}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.6\textwidth}
        \includegraphics[width=\textwidth]{img/vae_results/epoch_200.png}
        \caption{VAE reconstruction with 64-D latent space}
        \label{fig:vae64_recon}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.6\textwidth}
        \includegraphics[width=\textwidth]{img/vae_results/200_epochs_128_ls_logcosh/reconstructions/epoch_200.png}
        \caption{VAE reconstruction with 128-D latent space}
        \label{fig:vae128_recon}
    \end{subfigure}

    \caption[Reconstruction comparison for different VAE latent dimensions]{%
Qualitative comparison of reconstructed RGB images using different latent dimensionalities. The 64-dimensional model shows noticeable blurring and structural loss, while the 128-dimensional variant preserves edges and semantic layout more effectively.}
    \label{fig:vae_latent_qual_comparison}
\end{figure}

\begin{table}[!h]
    \centering
    \caption{Comparison of VAE latent dimensionalities.}
    \label{tab:vae-latent-comparison}
    \begin{tabular}{|c|p{4cm}|p{3.5cm}|p{4cm}|}
        \hline
        \textbf{Latent Dim} & \textbf{Reconstruction Quality} & \textbf{Training Stability} & \textbf{Latent Structure (t-SNE)} \\
        \hline
        64 & Blurry edges, missing details & Stable & Weak structure \\
        128 & Clear, high-fidelity images & Stable & Moderate clustering \\
        \hline
    \end{tabular}
\end{table}

The final VAE architecture design motivated from Idrees Shaikh’s CARLA DRL project\footnote{\url{https://github.com/idreesshaikh/Autonomous-Driving-in-Carla-using-Deep-Reinforcement-Learning}}, which originally employed semantic segmentation and a 64-dimensional latent space. Since this thesis works directly with raw RGB images, preserving color and fine-grained spatial detail required a richer representation. Accordingly, the latent dimension was increased to 128, and the architecture was fine-tuned through multiple iterations to ensure consistent convergence and high-quality reconstructions.

\clearpage



\subsection{Training Performance and Loss Analysis} \label{subsubsec:vae_training_loss}

Two VAE variants were trained for 200 epochs each (see hyperparameters in \cref{subsec:hyperparameter_config}), differing only in their reconstruction loss function: one using Mean Squared Error (MSE) and the other using Log-Cosh. Throughout training, several key performance metrics were monitored to evaluate model convergence, reconstruction quality, and the behavior of latent space regularization.

The \textbf{KL divergence} reflects how well the learned latent distribution aligns with the unit Gaussian prior, and its formulation is rooted in the VAE loss function discussed in \cref{subsec:VAE Objective Function and Loss} and \cref{subsec:reparameterization_trick}. The \textbf{reconstruction loss} measures pixel-wise error between original and generated images, directly reflecting the decoder's effectiveness and forming one half of the ELBO objective, as described in \cref{subsec:VAE Objective Function and Loss}. The \textbf{total loss}, combining reconstruction loss and KL divergence (with an annealing weight), serves as the main optimization target and is detailed in the ELBO formulation. 

In addition to these theoretical components, perceptual metrics were employed to evaluate the quality of generated images. The \textbf{Structural Similarity Index (SSIM)} captures human-perceived image similarity and complements traditional pixel-based losses. Similarly, the \textbf{Peak Signal-to-Noise Ratio (PSNR)} offers a quantitative measure of image fidelity. Finally, \textbf{pixel accuracy} assesses how closely the reconstructed image matches the original input on a pixel-wise level. These perceptual evaluations provide practical insights that go beyond the theoretical loss terms presented in \cref{sec:variational_inference}.

Figure~\ref{fig:vae_loss_comparison_1} and Figure~\ref{fig:vae_loss_comparison_2} provide a side-by-side comparison of these key metrics for both VAE models trained with Log-Cosh and MSE losses.

\begin{figure}[p]
    \centering
    % --- KL Divergence
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/vae_results/200_epochs_128_ls_logcosh/logcosh_kl_loss.png}
        \caption{KL Divergence — Log-Cosh}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/vae_results/200_epochs_128_ls_mse/mse_kl_loss.png}
        \caption{KL Divergence — MSE}
    \end{subfigure}

    % --- Reconstruction Loss
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/vae_results/200_epochs_128_ls_logcosh/logcosh_recon_loss.png}
        \caption{Reconstruction Loss — Log-Cosh}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/vae_results/200_epochs_128_ls_mse/mse_recon_loss.png}
        \caption{Reconstruction Loss — MSE}
    \end{subfigure}

    % --- Total Loss
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/vae_results/200_epochs_128_ls_logcosh/logcosh_total_loss.png}
        \caption{Total Loss — Log-Cosh}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/vae_results/200_epochs_128_ls_mse/mse_total_loss.png}
        \caption{Total Loss — MSE}
    \end{subfigure}

    \caption[Loss comparison for Log-Cosh and MSE-trained VAEs]{%
Comparison of KL divergence, reconstruction loss, and total loss across training epochs for VAE models trained with Log-Cosh and MSE losses. The Log-Cosh model shows more stable convergence with better structural recovery.}
    \label{fig:vae_loss_comparison_1}
\end{figure}


\begin{figure}[p]
    \centering
    % --- Pixel Accuracy
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/vae_results/200_epochs_128_ls_logcosh/logcosh_val_accuracy.png}
        \caption{Validation Accuracy — Log-Cosh}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/vae_results/200_epochs_128_ls_mse/mse_val_accuracy.png}
        \caption{Validation Accuracy — MSE}
    \end{subfigure}

    % --- PSNR
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/vae_results/200_epochs_128_ls_logcosh/logcosh_val_psnr.png}
        \caption{PSNR — Log-Cosh}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/vae_results/200_epochs_128_ls_mse/mse_val_psnr.png}
        \caption{PSNR — MSE}
    \end{subfigure}

    % --- SSIM
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/vae_results/200_epochs_128_ls_logcosh/logcosh_val_ssim.png}
        \caption{SSIM — Log-Cosh}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/vae_results/200_epochs_128_ls_mse/mse_val_ssim.png}
        \caption{SSIM — MSE}
    \end{subfigure}

    \caption[Performance metrics for Log-Cosh vs MSE VAEs]{%
Evaluation metrics for VAEs trained with Log-Cosh and MSE loss functions. Metrics include validation accuracy, PSNR (Peak Signal-to-Noise Ratio), and SSIM (Structural Similarity Index), showcasing better reconstruction fidelity for the Log-Cosh variant.}
    \label{fig:vae_loss_comparison_2}
\end{figure}

\clearpage

The comparative plots in Figure~\ref{fig:vae_loss_comparison_1} and Figure~\ref{fig:vae_loss_comparison_2} clearly demonstrate that the VAE trained with the Log-Cosh loss outperforms its MSE counterpart across all major evaluation criteria:

\begin{itemize}
    \item \textbf{KL Divergence:} Both models show stable convergence, but the Log-Cosh variant achieves a marginally lower KL loss by the end of training, indicating more effective regularization of the latent space (Figures~\ref{fig:vae_loss_comparison_1}a and~\ref{fig:vae_loss_comparison_1}b).
    
    \item \textbf{Reconstruction and Total Loss:} The Log-Cosh model exhibits faster convergence and lower final loss values, suggesting improved robustness during training (Figures~\ref{fig:vae_loss_comparison_1}c to~\ref{fig:vae_loss_comparison_1}f).
    
    \item \textbf{Pixel Accuracy:} As shown in Figures~\ref{fig:vae_loss_comparison_2}a and~\ref{fig:vae_loss_comparison_2}b, the Log-Cosh model consistently yields higher pixel accuracy, peaking at approximately 96.96\%.
    
    \item \textbf{PSNR and SSIM:} The perceptual quality of reconstructions is higher with Log-Cosh, achieving 31.04 dB PSNR and 0.864 SSIM (Figures~\ref{fig:vae_loss_comparison_2}c to~\ref{fig:vae_loss_comparison_2}f), compared to 29.44 dB and 0.823 with MSE.
\end{itemize}

Visual inspection further confirms these findings. Figure~\ref{fig:vae_qualitative_comparison} presents side-by-side reconstructions at epoch 200. The Log-Cosh model preserves finer details, textures, and structural integrity, whereas the MSE-based reconstruction exhibits noticeable blurring, particularly around object boundaries, disappearing of the traffic pole and fine features.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/vae_results/200_epochs_128_ls_logcosh/reconstructions/epoch_200.png}
        \caption{Log-Cosh — Epoch 200}
        \label{fig:logcosh_epoch200}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/vae_results/200_epochs_128_ls_mse/reconstructions/epoch_200.png}
        \caption{MSE — Epoch 200}
        \label{fig:mse_epoch200}
    \end{subfigure}
    \caption[Qualitative comparison of VAE reconstructions (Log-Cosh vs. MSE)]{%
Side-by-side qualitative comparison of VAE reconstructions at epoch 200. The Log-Cosh model (left) yields sharper and more semantically consistent outputs than the MSE model (right).}
    \label{fig:vae_qualitative_comparison}
\end{figure}







\clearpage


\subsection{Quantitative Comparison of Loss Functions (RQ2 Answered)} \label{subsec:vae_quant_comparison}
To assess the performance of the Variational Autoencoder (VAE), both the Log-Cosh and Mean Squared Error (MSE) loss functions were evaluated across several key metrics: Total Loss, Reconstruction Loss, KL Divergence, Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index (SSIM). The Total Loss is computed as the sum of the reconstruction loss (Log-Cosh or MSE) and the weighted Kullback-Leibler (KL) divergence, where the KL term regularizes the latent space and its contribution is gradually increased during training using KL annealing.

\begin{equation}
\mathcal{L}_{\text{VAE}} = \mathcal{L}_{\text{recon}} + \lambda_{\text{KL}} \cdot \mathcal{L}_{\text{KL}}
\end{equation}

Here, $\mathcal{L}_{\text{recon}}$ denotes the reconstruction loss (either Log-Cosh or MSE), $\mathcal{L}_{\text{KL}}$ is the KL divergence between the approximate posterior and the standard normal prior, and $\beta$ is a dynamic weight term that increases gradually during training to balance reconstruction fidelity with latent space regularization.

Table~\ref{tab:loss_comparison} presents average values over the final 5 epochs for both variants:

\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Metric} & \textbf{Log-Cosh (Avg)} & \textbf{MSE (Avg)} \\
        \midrule
        Validation Total Loss & 36.13 & 62.21 \\
        Validation Reconstruction Loss & 30.77 & 55.63 \\
        Validation KL Divergence & 270.11 & 331.46 \\
        Validation PSNR (dB) & 29.43 & 28.80 \\
        Validation SSIM & 0.8230 & 0.8088 \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of validation metrics for VAE variants (averaged over final 5 epochs).}
    \label{tab:loss_comparison}
\end{table}

Figures~\ref{fig:val_loss_comparison}, \ref{fig:val_psnr_comparison}, and \ref{fig:val_ssim_comparison} provide a visual comparison of how both losses influence model behavior throughout training.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{img/vae_results/val_loss_comparison.png}
    \caption{Validation Loss Comparison: Log-Cosh vs. MSE over all epochs.}
    \label{fig:val_loss_comparison}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{img/vae_results/val_psnr_comparison.png}
    \caption{Validation PSNR Comparison: Log-Cosh vs. MSE over all epochs.}
    \label{fig:val_psnr_comparison}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{img/vae_results/val_ssim_comparison.png}
    \caption{Validation SSIM Comparison: Log-Cosh vs. MSE over all epochs.}
    \label{fig:val_ssim_comparison}
\end{figure}

From these results:

\begin{itemize}
    \item \textbf{Figure~\ref{fig:val_loss_comparison}} shows that the Log-Cosh loss function results in faster and more stable convergence compared to MSE. After the initial 20 epochs, the loss curve for Log-Cosh remains consistently lower, confirming its robustness against outliers and noisy gradients.

    \item \textbf{Figure~\ref{fig:val_psnr_comparison}} demonstrates that the Log-Cosh variant yields higher PSNR throughout training. Since PSNR is a direct indicator of reconstruction quality, this suggests that reconstructions from the Log-Cosh model are closer to the original inputs in pixel space.

    \item \textbf{Figure~\ref{fig:val_ssim_comparison}} provides further evidence that the Log-Cosh loss leads to more perceptually realistic outputs. The SSIM curve, which accounts for luminance, contrast, and structural similarity, consistently favors Log-Cosh, particularly in the later epochs, indicating better structural retention in the reconstructions which is clearly shown in the plot blue line indicated the log-cosh loss ssim and orange indicates the mse loss ssim.

    \item Additionally, the KL divergence is lower in the Log-Cosh variant (Table~\ref{tab:loss_comparison}), indicating a better trade-off between regularization and reconstruction fidelity.
\end{itemize}

Overall, these findings confirm that the Log-Cosh loss not only provides better quantitative performance but also leads to reconstructions that are perceptually more accurate and visually coherent. This clearly answers RQ2 by validating Log-Cosh as the superior choice for training VAEs in this context.


\clearpage


\subsection{Visual Comparison of Reconstructions} \label{subsubsec:vae_visual_recon}
Representative input images and corresponding reconstructions from both models at epoch 200 were visually compared.

Log-Cosh Reconstructions (Figure~\ref{fig:logcosh_epoch200}) retain road boundaries, tree textures, and lighting gradients with greater smoothness and fewer artifacts.

MSE Reconstructions (Figure~\ref{fig:mse_epoch200}) exhibit noise in edge regions and less coherent texture patterns and lost many important details like traffic light pole, road boundary and many more.

These qualitative results further confirms that adopting the Log-Cosh loss, as motivated by Chen et al.~\cite{chen2019log}, substantially improves VAE performance thereby answering RQ2 (How can loss function modifications in Variational Autoencoders (VAEs) optimize image reconstruction quality in autonomous driving tasks?).


%\vspace{1em}
\subsection{Latent Space Visualization and Separability}
\label{subsubsec:vae_latent_space}

Although the VAE in this thesis is trained in an unsupervised manner without using class labels, one of its key objectives is to learn a latent space that not only enables accurate image reconstruction but also supports downstream classification and interpretability. To examine this, the latent vectors produced by the encoder were evaluated using a separate classifier trained on top of these vectors. Despite the lack of supervision during VAE training, the classifier demonstrated strong performance, indicating that the learned latent representations capture semantically meaningful and class-relevant features.

To qualitatively assess the structure of the latent space, dimensionality reduction techniques were applied to the 128-dimensional latent vectors extracted from the test set. These vectors were projected into two dimensions using Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE). The resulting embeddings are shown in Figure~\ref{fig:latent_space_visualizations}, where each point is colored according to its ground-truth class label.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{img/vae_results/media_images_PCA Latent_0_1f2a94b6feecab3585be.png}
        \caption{PCA projection of latent vectors (by true labels).}
        \label{fig:pca_true}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{img/vae_results/media_images_t-SNE Latent_1_77cd107489e31c6c9f4d.png}
        \caption{t-SNE projection of latent vectors (by true labels).}
        \label{fig:tsne_true}
    \end{subfigure}
    \caption[Latent space projections using PCA and t-SNE]{%
Latent space projections using PCA and t-SNE for test set samples. Each point represents an encoded image projected to 2D. Colors denote ground-truth labels: Class 0 = \texttt{STOP}, Class 1 = \texttt{GO}, Class 2 = \texttt{LEFT}, Class 3 = \texttt{RIGHT}.}
    \label{fig:latent_space_visualizations}
\end{figure}


These visualizations illustrate the extent to which semantically similar driving scenes are grouped in the latent space. While some local clustering is visible—particularly for the \texttt{STOP} (Class 0) and \texttt{RIGHT} (Class 3) classes—the overall class separability is limited. Notably, the \texttt{GO} (Class 1) and \texttt{LEFT} (Class 2) classes show significant overlap, which may reflect their visual similarity in real-world scenarios such as urban intersections.

This observation reflects a core tradeoff in VAE training: while the KL divergence term encourages smooth and continuous latent distributions, it can also limit the formation of sharply separable class clusters—especially when such clusters do not emerge naturally from the reconstruction objective alone.

Importantly, since the VAE was trained entirely without labels, the latent space structure observed here arises organically from its goal of reconstructing input images while conforming to a Gaussian prior. As such, latent space visualizations and separability metrics serve as useful tools for evaluating the VAE’s representational capacity and its suitability for downstream tasks, including classification and counterfactual explanation generation discussed in later sections.












\clearpage



\section{Evaluation of Classifiers Trained on Latent Features} \label{sec:classifier_eval}

Following the successful design and training of the Variational Autoencoder (VAE), the next step in this thesis was to assess the effectiveness of its learned latent space in enabling downstream decision-making. Specifically, this section evaluates whether the encoded 128-dimensional latent representations can support accurate multi-class classification of driving actions. This directly addresses the core objective of \textbf{RQ1}, by investigating the semantic richness and discriminative power of the latent vectors produced by the VAE.

To this end, a set of traditional and neural classifiers were trained and evaluated on the same VAE-derived latent representations, using a balanced four-class dataset comprising \texttt{STOP}, \texttt{GO}, \texttt{LEFT}, and \texttt{RIGHT} labels. Five models were studied: Logistic Regression (baseline linear classifier), K-Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Neural Network (NN).

Classifier training implementation details are described in \cref{sec:classifier_architectures}. Performance evaluation was carried out using a comprehensive set of metrics such as accuracy, per-class precision, recall, F1-score, macro and weighted averages, confusion matrices, and ROC-AUC curves for one-vs-rest classification~\cite{labelf2025metrics, roc_auc2024, svm_rf_knn2017}.



\subsection{Evaluations and Results of Traditional Classifiers} \label{subsec:comparision_with_traditional_classifiers}

To contextualize the performance of the neural model, four traditional classifiers were evaluated using the same latent vectors. Training and implementations of these classifiers are elaborated in \cref{sec:classifier_architectures}. Figure~\ref{fig:comparison_matrices} summarizes the confusion matrices for each model, and Figure~\ref{fig:comparison_rocs} provides the corresponding ROC curves.

\begin{figure}[h]
\centering
\begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\linewidth]{img/classifier/logistic_regression_confucion_matrix.png}
    \caption{Logistic Regression}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\linewidth]{img/classifier/KNN_confucion_matrix.png}
    \caption{KNN}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\linewidth]{img/classifier/random_forest_confucion_matrix.png}
    \caption{Random Forest}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\linewidth]{img/classifier/SVM_confucion_matrix.png}
    \caption{SVM}
\end{subfigure}

\caption{Confusion matrices for traditional classifiers.}
\label{fig:comparison_matrices}
\end{figure}


\begin{figure}[h]
\centering
\begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\linewidth]{img/classifier/logistic_regression_AUC.png}
    \caption{Logistic Regression}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\linewidth]{img/classifier/KNN_AUC.png}
    \caption{KNN}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\linewidth]{img/classifier/random_forest_AUC.png}
    \caption{Random Forest}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\linewidth]{img/classifier/SVM_AUC.png}
    \caption{SVM}
\end{subfigure}

\caption{ROC curves and AUC values for traditional classifiers (1-vs-rest).}
\label{fig:comparison_rocs}
\end{figure}





\paragraph{Logistic Regression.}

The Logistic Regression classifier achieved an overall accuracy of 80\% and a macro F1-score of 0.80. As seen in Figure~\ref{fig:comparison_matrices}(a), the confusion matrix reveals that the model performs well for the \texttt{STOP} and \texttt{LEFT} classes, while showing significant misclassification for the \texttt{GO} and \texttt{RIGHT} classes. Notably, many \texttt{GO} instances were misclassified as \texttt{RIGHT} or \texttt{LEFT}, highlighting the difficulty in linearly separating these features in the latent space.

The ROC curves in Figure~\ref{fig:comparison_rocs}(a) further support this observation. While the \texttt{STOP} and \texttt{LEFT} classes achieved high AUC scores (0.98 and 0.96 respectively), the \texttt{GO} class recorded the lowest AUC at 0.88, indicating a higher overlap with other classes in the latent space.

Overall, Logistic Regression acts as a valuable baseline. Its performance confirms the need for non-linear classifiers to fully leverage the representational power of the VAE's latent space.

\paragraph{K-Nearest Neighbors (KNN).}

The K-Nearest Neighbors classifier achieved an overall accuracy of 88\% and a macro F1-score of 0.88, demonstrating competitive performance on the latent feature space extracted by the VAE. As shown in Figure~\ref{fig:comparison_matrices}(b), the model achieved near-perfect recall for the \texttt{STOP} class (99\%) and high precision for both \texttt{STOP} and \texttt{RIGHT}.

The most significant confusion occurred between the \texttt{LEFT} and \texttt{GO} classes, where 92 instances of \texttt{LEFT} were incorrectly predicted as \texttt{GO}, likely due to local similarity in features. This aligns with the proximity-based nature of KNN, which is sensitive to cluster overlap in high-dimensional spaces.

Figure~\ref{fig:comparison_rocs}(b) shows the ROC curves, where all classes achieve AUC values above 0.90. \texttt{STOP} reached the highest AUC of 0.99, followed by \texttt{LEFT} (0.95), \texttt{RIGHT} (0.94), and \texttt{GO} (0.91).

In summary, KNN offers a strong non-parametric baseline that effectively captures local structure in the VAE's latent space, though it may require further tuning or metric learning to fully disambiguate overlapping action classes.


\paragraph{Random Forest.}
The Random Forest classifier achieved an accuracy of 88\% and a macro F1-score of 0.88. As shown in Figure~\ref{fig:comparison_matrices}(c), the confusion matrix indicates strong performance across most classes, particularly for \texttt{STOP} (F1 = 0.95) and \texttt{RIGHT} (F1 = 0.89). The model also demonstrated competitive recall for the \texttt{GO} class (0.87), which often presents the most overlap with other categories.

ROC analysis (Figure~\ref{fig:comparison_rocs}(c)) revealed high AUC scores across the board, with \texttt{STOP} achieving a perfect AUC of 1.00, and the other classes maintaining AUCs above 0.95. These findings affirm the Random Forest’s ability to exploit the structure of the latent space effectively.

Complete classification metrics for all four classes are provided in Appendix~\ref{tab:random_forest_classification_report}, which reinforce the model’s balanced precision and recall across classes.

\paragraph{Support Vector Machine (SVM).}
The SVM classifier achieved an overall accuracy of 89\% and a macro F1-score of 0.89, outperforming several traditional models. As seen in Figure~\ref{fig:comparison_matrices}(d), the confusion matrix reveals strong and consistent performance across all classes. The model performed especially well for \texttt{STOP} and \texttt{LEFT} classes with F1-scores of 0.93 and 0.92 respectively, and demonstrated solid performance for \texttt{RIGHT} (F1 = 0.90). While the \texttt{GO} class had slightly lower scores (F1 = 0.81), it still surpassed performance seen in Logistic Regression and KNN.

The ROC curves in Figure~\ref{fig:comparison_rocs}(d) further support this, showing high AUC values across all classes, each reaching or exceeding 0.94. This indicates excellent class separability in the latent space when using a margin-based classifier with non-linear kernels.

A detailed breakdown of the classification metrics is available in Appendix~\ref{tab:svm_classification_report}, demonstrating that SVMs can leverage the non-linear separability embedded in the latent representations learned by the VAE.




Table~\ref{tab:classifier_comparison} presents a side-by-side comparison of accuracy and F1-scores across all classifiers:

\begin{table}[htbp]
\centering
\small
\begin{tabular}{p{3cm}ccp{5.5cm}}
\toprule
\textbf{Classifier} & \textbf{Accuracy} & \textbf{Macro F1} & \textbf{Notes} \\
\midrule
Logistic Regression & 80\% & 0.80 & Baseline linear classifier; struggled with \texttt{GO} \\
KNN & 88\% & 0.88 & High recall for \texttt{STOP}; sensitive to latent proximity \\
SVM & 89\% & 0.89 & Excellent class separability; strong on \texttt{LEFT} and \texttt{STOP} \\
Random Forest & 88\% & 0.88 & Robust performance; most errors in \texttt{GO} class \\
Neural Network & \textbf{89\%} & \textbf{0.89} & Best overall F1 balance; handles non-linear separations \\
\bottomrule
\end{tabular}
\caption{Performance comparison of classifiers trained on VAE latent features (4-class).}
\label{tab:classifier_comparison}
\end{table}


Across all classifiers, the \texttt{GO} class consistently had the lowest precision and recall. This suggests latent encodings for \texttt{GO} overlap more with other classes, possibly due to its transitional visual nature. Addressing this may require additional context, or spatial features.


\subsection{Evaluation of Neural Network Classifier (MLP)}
\label{subsec:classifier_trained_on_latentfeatures}

The neural network classifier, implemented as a Multi-Layer Perceptron (MLP), was trained for 20 epochs on the 128-dimensional latent features extracted from the VAE encoder. This model aimed to capture the non-linear relationships within the latent space and serve as a powerful baseline for downstream classification tasks.

Training progression is visualized in Figure~\ref{fig:loss_accuracy_plot}, which shows the learning dynamics over the course of training. The left plot demonstrates a consistent reduction in loss, while the right plot shows a steady increase in training accuracy, reaching a final value of approximately 93\%. These curves indicate effective convergence without overfitting, affirming the model's ability to generalize well on latent representations.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{img/classifier/training_loss_accuracy_4_classes.png}
    \caption[Training performance of 4-class neural classifier]{%
Training loss and accuracy curves for the neural classifier across 20 epochs. The plot illustrates the model's convergence and learning dynamics during training.}
    \label{fig:loss_accuracy_plot}
\end{figure}

Quantitative evaluation results on the test set are presented in Table~\ref{tab:classification_report}. The MLP achieved an overall accuracy of 89\% and a macro-averaged F1-score of 0.89, demonstrating robust performance across all four driving behavior classes.

\begin{table}[h]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Support} \\
        \midrule
        STOP & 0.91 & 0.99 & 0.95 & 821 \\
        GO & 0.81 & 0.84 & 0.83 & 821 \\
        RIGHT & 0.93 & 0.86 & 0.89 & 821 \\
        LEFT & 0.93 & 0.89 & 0.91 & 821 \\
        \midrule
        \textbf{Accuracy} & \multicolumn{4}{c}{0.89 (3284 samples)} \\
        \textbf{Macro avg} & 0.90 & 0.89 & 0.89 & -- \\
        \textbf{Weighted avg} & 0.90 & 0.89 & 0.89 & -- \\
        \bottomrule
    \end{tabular}
    \caption{Classification report for the neural network trained on latent features (4-class).}
    \label{tab:classification_report}
\end{table}

The confusion matrix in Figure~\ref{fig:conf_matrix} provides additional insights into the model's performance by revealing class-specific error patterns. The \texttt{STOP} class is classified with the highest precision and recall, while the \texttt{GO} class remains the most challenging due to its visual and contextual overlap with other classes.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{img/classifier/confusion_matrix_4_classes.png}
    \caption[Confusion matrix of neural classifier (4-class)]{%
Confusion matrix of the neural classifier on the 4-class test set. The matrix highlights per-class prediction performance and any misclassification trends.}
    \label{fig:conf_matrix}
\end{figure}

\begin{itemize}
    \item \textbf{STOP:} Most accurately classified class (Recall = 0.99), confirming its strong separability in the latent space.
    \item \textbf{GO:} Lowest-performing class with Precision = 0.81 and Recall = 0.84. Common misclassifications include confusion with \texttt{RIGHT} and \texttt{LEFT}.
    \item \textbf{RIGHT:} High precision (0.93) and moderate recall (0.86). Some misclassification occurs with the \texttt{GO} class.
    \item \textbf{LEFT:} Balanced performance with strong metrics across all dimensions, but minor confusion with \texttt{GO} was observed.
\end{itemize}

These results suggest that the learned latent space is semantically meaningful and well-structured, enabling accurate and efficient multi-class classification. The MLP outperformed all traditional classifiers, confirming the hypothesis that non-linear models are better suited for leveraging the representational richness of the VAE-derived latent features. This conclusion directly supports the research question RQ1 regarding the effectiveness of latent representations for downstream prediction.

\begin{itemize}
    \item \textbf{Neural Classifier and SVM} emerged as top performers, both achieving an F1-score of 0.89 and high recall for critical classes such as \texttt{STOP} and \texttt{LEFT}.
    \item \textbf{Random Forest} performed competitively, with strong recall on \texttt{STOP} (0.98), but showed minor confusion on \texttt{GO} and \texttt{LEFT}.
    \item \textbf{KNN} offered fast implementation with solid results, but is sensitive to feature scaling and suffered from overlap in ambiguous classes like \texttt{GO}.
    \item \textbf{Logistic Regression} showed the weakest performance (Accuracy = 80\%), revealing the limitations of linear separability in the VAE's latent space.
\end{itemize}

\subsection{Discussion of Classifier Results}

This comprehensive evaluation confirms that the latent space learned by the VAE is highly effective for downstream classification tasks. All classifiers achieved reasonable performance with macro F1-scores above 0.80. However, the Neural Network (MLP) classifier consistently outperformed other models in terms of both accuracy and generalization across all four driving classes. With an F1-score of 0.89 and robust recall on all classes—including the more ambiguous ones like GO and LEFT the MLP demonstrated superior capability in modeling the non-linear structure of the VAE's latent space.

While SVM and Random Forest showed competitive performance, their training flexibility and scalability were limited compared to the MLP, especially when extended to gradient-based counterfactual explanation methods. KNN and Logistic Regression served as valuable baselines but lacked the expressive power required for capturing complex class boundaries in the latent space.

Therefore, the MLP classifier was selected for all downstream experiments and counterfactual explanation tasks throughout this thesis. Its deep architecture, non-linear activation functions, and regularization mechanisms make it particularly well-suited for high-dimensional, semantically rich latent features generated by the VAE. This choice ensures consistency, interpretability, and compatibility with the explanation frameworks introduced in later chapters.

These findings empirically support the hypothesis posed in RQ1 that deep generative models like VAEs can produce compact yet semantically meaningful representations of high-dimensional driving scenes representations that can be effectively leveraged for interpretable and reliable decision-making pipelines.





\vspace{1em}

\section{Evaluation of Counterfactual Explanation Generation via Masking Techniques (RQ3)} \label{sec:masking_eval}
To address RQ3, we evaluate the quality and efficiency of counterfactual explanations generated using multiple feature masking techniques which are explained in the \cref{sec:feature_masking_pipeline}. Each method is assessed using metrics commonly adopted in counterfactual explanation literature~\cite{DELANEY2023103995, chan2022comparativestudyfaithfulnessmetrics, Singh1622975, MARKUS2021103655, DBLP:journals/corr/abs-1905-07697}, including Validity, Runtime, Sparsity, Proximity, Method Overlap, and Failure Rate. These metrics capture both the effectiveness (e.g., class change, semantic integrity, sparsity of perturbation) and efficiency (e.g., runtime, proximity) of the generated counterfactuals.
All experiments use the same VAE encoder and classifier checkpoints and are performed on identical data splits to ensure consistency and fair comparison across masking methods.
While object detection masking was initially considered, it was excluded from overlap analysis due to low detection rates and missed regions in the current dataset, resulting in poor counterfactual explanation coverage.

\subsection{Counterfactual Generation Coverage}
We first measured the number of successful counterfactuals generated by each masking method across the binary and multi-class datasets. Figures~\ref{fig:ce_count_binary} and \ref{fig:bar_chart_ce_count_multi} illustrate the total number of counterfactual explanations generated by each method, respectively.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/masking_results/bar_chart_explanations_2_class.png}
        \caption{CE count by method (2-class setup).}
        \label{fig:ce_count_binary}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/masking_results/bar_chart_explanations_4_class.png}
        \caption{CE count by method (4-class setup).}
        \label{fig:bar_chart_ce_count_multi}
    \end{subfigure}
    \caption[Number of counterfactuals found by method (binary vs multi-class)]{%
Comparison of the number of successful counterfactual explanations generated by different masking methods across (a) the binary-class and (b) multi-class classification setups. Each bar represents the total count of counterfactuals found using that method.}
    \label{fig:ce_count_comparison}
\end{figure}


As shown in these figures, LIME on Latent NUN outperforms other methods in binary class and grid based maskning in multi-class settings. It successfully explained 2,298 out of 2,422 images (94.88\%) in the binary setup. While 2,395 images out of 2422 images (98.88\%) in the 4-class case. In contrast, LIME on Latent feature masking using median values shows the lowest performance, with only 6.98\% and 12.26\% success in the binary and multi-class settings, respectively.

\subsection{Masking Method Overlap} \label{subsubsec:masking_method_overlap}
To understand how these methods complement one another, we analyzed the overlaps in counterfactual explanations using Venn diagrams.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/masking_results/venn_2_class.png}
        \caption{Method overlap (2-class).}
        \label{fig:venn_binary}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/masking_results/venn_4_class.png}
        \caption{Method overlap (4-class).}
        \label{fig:venn_multi}
    \end{subfigure}
    \caption[Overlap of counterfactuals per method using Venn diagrams]{%
Venn diagrams showing the overlap of counterfactual explanations generated by different masking methods in the (a) 2-class and (b) 4-class setups. These visualizations highlight where methods produce unique or shared explanations.}
    \label{fig:venn_comparison}
\end{figure}



From Figure~\ref{fig:venn_binary}, we observe:
\begin{itemize}
    \item 808 images (33.36\%) are explained by Grid-Based, LIME on Images, and LIME on Latent NUN masking methods together.
    \item 110 images (4.54\%) are explained by all four methods.
    \item 659 images (27.21\%) are exclusively explained by LIME on Latent NUN masking method.
\end{itemize}

Similarly, Figure~\ref{fig:venn_multi} reveals:
\begin{itemize}
    \item 1,820 images (75.14\%) are explained by Grid-Based, LIME on Images, and LIME on Latent NUN together.
    \item Only 249 images (10.28\%) are consistently explained by all four techniques.
\end{itemize}

These results indicate that while LIME on Latent feature masking contributes marginally when used alone, the LIME on Latent features masking using NUN method technique not only achieves broad coverage but also offers unique explanations not captured by other methods.


\subsection{Validity of Counterfactual Explanations}
As discussed in section~\ref{subsec:background_desirable_properties_of_CEs}, validity is a crucial metric for assessing counterfactual explanations. Table~\ref{tab:ce_validity} presents the percentage of valid counterfactuals per masking method---i.e., cases where the predicted class changed post-masking while maintaining high-quality reconstruction. In this work, a counterfactual is considered valid if it alters the model’s output to the desired target class (e.g., from \texttt{STOP} to \texttt{GO}, or from \texttt{RIGHT} to \texttt{LEFT}).

The validity percentage is computed using the following formula:

\[
\text{Validity (\%)} = \left( \frac{\text{Successful Counterfactuals}}{\text{Total Counterfactuals}} \right) \times 100.
\]

\begin{table}[htbp]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Binary CE Validity (\%)} & \textbf{Multi-Class CE Validity (\%)} & \textbf{Interpretation} \\
\midrule
Grid-Based Masking & 70.52 & 98.89 & Robust and fast; excellent for multi-class \\
LIME on Latent Maksing   & 6.98  & 12.26 & Limited effect, likely due to poor latent locality \\
LIME on Image Maksing    & 38.32 & 87.65 & Effective but slower due to pixel masking \\
LIME on Latent NUN& \textbf{94.88} & \textbf{97.44} & Highest performance and generalizability \\
\bottomrule
\end{tabular}%
}
\caption{Comparison of counterfactual explanation validity across masking methods.}
\label{tab:ce_validity}
\end{table}

As seen in Table~\ref{tab:ce_validity}, the LIME on Latent NUN method achieves the highest validity, confirming its effectiveness in producing meaningful and causally influential perturbations. This supports its alignment with the theoretical definition of a "valid counterfactual" described in the background (Section~\ref{subsec:background_desirable_properties_of_CEs}). On the other hand, traditional LIME on Latent (Median) shows poor validity despite being sparse indicating that minimal changes do not necessarily result in effective counterfactuals unless they target truly causal dimensions.



\subsection{Sparsity and Proximity Analysis}

Sparsity and proximity are important dimensions of interpretability, as discussed in Section~\ref{subsec:background_desirable_properties_of_CEs}. Sparsity refers to the number of features or regions modified to achieve a prediction change, while proximity quantifies the similarity between the original and counterfactual instances, typically using the L2 distance in latent space. These two metrics jointly reflect how minimally and plausibly a counterfactual modifies the original input—a crucial consideration for trust and usability in real-world AI systems~\cite{Singh1622975}.

In this work, sparsity was defined based on the domain in which the masking was applied. For latent space methods such as LIME on Latent (Median) and LIME on Latent using NUN, sparsity was computed as the number of latent dimensions replaced during counterfactual generation. These values were logged in the output files under the “Features Replaced” column. For Grid-Based Masking, sparsity was computed as the number of image grid patches masked. These grid indices were explicitly stored in the results logs\footnote{\url{https://github.com/darshandodamani/A-Counterfactual-Explanation-Approach-Using-Deep-Generative-Models/tree/main/results/masking}} and retrieved to compute average sparsity per instance. In the case of LIME on Image, sparsity was assigned a fixed value of 128, as all superpixels selected by LIME were masked at once, representing maximum perturbation across the image space.

Proximity, on the other hand, was computed uniformly across all methods in the latent space. Specifically, it was calculated as the Euclidean (L2) distance between the original latent vector $\mathbf{z}$ and the re-encoded counterfactual latent vector $\mathbf{z'}$, as follows:

\[
\text{Proximity} = \|\mathbf{z} - \mathbf{z'}\|_2
\]

This ensures that comparisons are fair and consistent, regardless of whether the masking was applied in the image space or the latent space. Since classification occurs in the latent space, this choice of proximity metric directly reflects how much the latent semantics change as a result of the intervention.

\vspace{1em}
\begin{table}[htbp]
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Avg. Sparsity} & \textbf{Avg. Proximity} & \textbf{Coverage (\%)} & \textbf{Failure Rate (\%)} \\
\midrule
Grid-Based Masking       & 60.2    & 5.9   & 98.89 & 1.11 \\
LIME on Latent (Median)  & \textbf{2.3}     & \textbf{0.25}  & 12.26 & 87.74 \\
LIME on Latent NUN       & 75.3    & 7.4   & 97.44 & 2.56 \\
LIME on Image            & 128.0   & 25.2  & 87.65 & 12.35 \\
\bottomrule
\end{tabular}
\caption{Average sparsity and proximity of successful counterfactuals across masking methods (multi-class setup).}
\label{tab:sparsity_proximity}
\end{table}

LIME on Latent (Median) stands out as the sparsest method, requiring minimal intervention to generate counterfactuals. However, its limited effectiveness is reflected in poor CE coverage and high failure rate (Table~\ref{tab:ce_validity}), indicating that overly simplistic changes may fail to achieve meaningful prediction flips. In contrast, LIME on Latent NUN modifies more features on average, yet offers significantly better validity and class coverage, highlighting its ability to generate targeted and semantically rich counterfactuals.

As described earlier in Section~\ref{subsec:background_desirable_properties_of_CEs}, low proximity is desirable as it implies that the counterfactual remains close to the original instance, preserving realism and interpretability. From this perspective, LIME on Latent (Median) shows the best proximity score, but again, with the trade-off of low success rate. Grid-Based Masking and LIME on Latent NUN offer a better balance, providing competitive proximity while maintaining high coverage and success rates.

It is worth noting that while sparsity and proximity are reliable metrics for latent-space interventions, they are less meaningful in image space due to the nature of VAE reconstruction. For image-based methods like LIME on Image or Grid-Based Masking, masked regions can introduce artifacts or out-of-distribution distortions, which propagate through the encoder and affect proximity values. As a result, although proximity is consistently reported for all methods, its interpretability is strongest for latent-based techniques, where both the intervention and evaluation occur in a structured and learned representation space.

These differences are further discussed in Section~\ref{subsubsec:qualitative_examples}, where qualitative visualizations help clarify the implications of masking induced distortions and their influence on semantic plausibility. Overall, sparsity and proximity remain essential for comparing methods, but must be interpreted in the context of where the intervention occurs and how reconstruction artifacts might affect the underlying latent semantics.


\subsection{Per-Class Counterfactual Explanantions Success}
Table~\ref{tab:classwise_ce_multi} reports the number of valid counterfactuals generated per class.

\begin{table}[htbp]
\centering
\scriptsize
\begin{tabular}{lcccccccc}
\toprule
\textbf{Method} & \textbf{STOP (\%)} & \textbf{GO (\%)} & \textbf{LEFT (\%)} & \textbf{RIGHT (\%)} & \textbf{Total CE Found} & \textbf{Total CE (\%)} & \textbf{Total Time (min)} \\
\midrule
Grid-Based        & 100.0 & 100.0 & 89.0  & 100.0 & 2395 & 98.89 & 12.98 \\
LIME on Latent    & 10.3  & 9.4   & 18.4  & 26.1  & 297  & 12.26 & 92.63 \\
LIME on Image     & 100.0 & 100.0 & 1.6   & 74.5  & 2123 & 87.65 & 78.65 \\
LIME on Latent NUN & 98.6  & 96.0  & 97.2  & 99.6  & \textbf{2360} & \textbf{97.44} & 263.79 \\
\bottomrule
\end{tabular}
\caption{Per-class counterfactual explanation success (multi-class setup).}
\label{tab:classwise_ce_multi}
\end{table}

The table shows that while most methods effectively capture STOP and GO transitions, LIME on Latent NUN exhibits the most balanced performance across all classes, closely followed by the Grid-Based method.




\vspace{1em}


\subsection{Efficiency: Time Complexity and Execution Time}
As discussed in the Background (see Section~\ref{subsec:background_desirable_properties_of_CEs}), one of the desirable properties of a counterfactual explainer is \textit{efficiency} the ability to generate explanations within a reasonable time, particularly in real-time or interactive systems like autonomous driving. In this evaluation (Section~\ref{sec:masking_eval}), we observe significant variation in execution times across the proposed methods.

While the LIME on Latent NUN approach consistently produced the highest-quality and most valid counterfactuals, it was also the most computationally expensive, requiring over 263 minutes to process the entire dataset. In contrast, Grid-Based Masking achieved nearly equivalent performance in terms of coverage and validity but completed in just 13 minutes. This highlights the trade-off between explanation quality and computational efficiency—an important consideration when selecting methods for deployment in time-sensitive applications.



\subsection{Qualitative Examples of Counterfactual Explanations} \label{subsubsec:qualitative_examples}
To complement the quantitative evaluation of counterfactual explanation using different masking methods, qualitative visual analyses were conducted to assess the visual plausibility, semantic preservation, and discriminative alterations introduced by each masking technique. These visual examples help contextualize how each method perturbs the input to find counterfactual explanations that lead to prediction changes.

Individual counterfactual examples for all five masking methods Grid-Based Masking, Object Detection-Based Masking, LIME on Image, LIME on Latent Features, and LIME on Latent using NUN are provided in \cref{sec:feature_masking_pipeline} (see Figures~\ref{fig:grid_ce_example}--\ref{fig:object_detection_masking}). Each figure narratively presents:

\begin{itemize}
    \item The original input image and its predicted class,
    \item The type of masking applied (e.g., grid, object, latent),
    \item The reconstructed image after masking via the VAE,
    \item The change in prediction resulting from the perturbation.
\end{itemize}

This structure highlights the causally relevant regions identified by each method and their impact on the classification outcome.

Each method is qualitatively evaluated based on:
\begin{itemize}
    \item Semantic fidelity: Whether the essential image structure is preserved after masking and reconstruction.
    \item Visual realism: The plausibility of the counterfactual image.
    \item Effectiveness: Whether the counterfactual successfully changes the predicted class.
\end{itemize}

\vspace{0.5em}

To consolidate these comparisons, Table~\ref{tab:cf_visual_examples} provides an overview of one representative counterfactual explanation per method. These samples were selected from the test set where class flips occurred, illustrating how each masking technique alters the input to change the classifier's decision.

\begin{table}[htbp]
    \centering
    \caption[Counterfactual examples across masking methods]{%
Representative counterfactual explanations across different masking methods. In each case, the original image is labeled \textbf{STOP}, and the counterfactual image is labeled \textbf{GO} due to the masking or removal of the STOP sign or related features.}
    \label{tab:cf_visual_examples}
    \begin{tabular}{>{\centering\arraybackslash}p{3.5cm} >{\centering\arraybackslash}c >{\centering\arraybackslash}c}
        \toprule
        \textbf{Masking Method} & \textbf{Original Image (STOP)} & \textbf{Counterfactual (GO)} \\
        \midrule
        Grid-Based Masking & 
        \includegraphics[width=0.3\textwidth]{img/masking_results/original.png} & 
        \includegraphics[width=0.3\textwidth]{img/masking_results/grid_cf.png} \\
        \addlinespace
        Object Detection Masking & 
        \includegraphics[width=0.3\textwidth]{img/masking_results/original.png} & 
        \includegraphics[width=0.3\textwidth]{img/masking_results/object_detection_cf.png} \\
        \addlinespace
        LIME on Image & 
        \includegraphics[width=0.3\textwidth]{img/masking_results/original.png} & 
        \includegraphics[width=0.3\textwidth]{img/masking_results/lime_image_cf.png} \\
        \addlinespace
        LIME on Latent & 
        \includegraphics[width=0.3\textwidth]{img/masking_results/original.png} & 
        \includegraphics[width=0.3\textwidth]{img/masking_results/lime_latent_cf.png} \\
        \addlinespace
        LIME on Latent NUN & 
        \includegraphics[width=0.3\textwidth]{img/masking_results/original.png} & 
        \includegraphics[width=0.3\textwidth]{img/masking_results/lime_NUN_cf.png} \\
        \bottomrule
    \end{tabular}
\end{table}


\textbf{Observations:}
\begin{itemize}
    \item \textbf{Grid-Based Masking:} Often introduces localized changes with some visible artifacts but successfully flips the class in most cases. It demonstrates good semantic targeting, although the visual naturalness may sometimes be compromised.
    \item \textbf{Object Detection Masking:} Attempts to remove salient objects. However, due to limitations in YOLOv5's detection performance on this dataset, many regions are missed. This leads to lower counterfactual success rates and less informative counterfactuals. Object detection masking is retained here primarily for completeness.
    \item \textbf{LIME on Image:} Produces counterfactuals that are visually intuitive by masking semantically important regions (e.g., road signs, motion cues). Although the reconstructions are perceptually realistic, the region selection can sometimes lack subtlety.
    \item \textbf{LIME on Latent:} Directly alters internal representations, often resulting in blurry or implausible reconstructions. This can lead to noisy or confusing counterfactuals that fail to reliably flip the class.
    \item \textbf{LIME on Latent NUN:} Achieves the best balance by minimally perturbing only the most relevant latent features. Its counterfactuals are both semantically meaningful and visually realistic, leading to successful class flips with minimal distortion.
\end{itemize}

\vspace{1em}

Together, these qualitative comparisons reinforce the quantitative results presented earlier (Figures~\ref{fig:bar_chart_ce_count_multi}--\ref{fig:venn_comparison}). In particular, LIME on Latent based masking using NUN method consistently generates the most faithful and effective counterfactual explanations. The visual clarity and semantic integrity of its outputs further confirm its practical utility for explainability in autonomous driving models.


\subsection{Discussions on Counterfactual Explanation Generation via Masking Techniques}

The experimental results provide a comprehensive comparison of the five masking-based counterfactual generation techniques and highlight clear trends in their relative strengths and limitations. Among all evaluated methods, LIME-guided latent feature masking using Nearest Unlike Neighbor (NUN) emerged as the most effective approach. It consistently produced high-quality counterfactuals that were both class-discriminative and semantically aligned with the original input, demonstrating superior performance across nearly all evaluation metrics. However, this method comes with a notable trade-off—its computational cost. With an average runtime exceeding 263 minutes for the full dataset, it poses challenges for deployment in time-sensitive or real-time environments.

Grid-Based Masking, on the other hand, offered a compelling alternative. While slightly less effective in terms of semantic richness and interpretability, it excelled in coverage, speed, and visual coherence. With an execution time of just 13 minutes and near-perfect class-wise counterfactual success, it serves as a strong baseline that balances performance and practicality, especially for multi-class prediction tasks in autonomous driving.

In contrast, LIME-based masking on latent features (with median replacement) exhibited significant limitations. Although it achieved the lowest average sparsity changing as few as 2.3 latent dimensions per counterfactual—it suffered from poor validity and limited coverage. This suggests that minimal changes alone are not sufficient if they fail to meaningfully influence the classifier's decision boundary. Similarly, LIME on Image, while more successful in terms of coverage, produced counterfactuals with very high sparsity (treated as 128), resulting in distorted reconstructions and poor interpretability. The primary issue here stems from the masking strategy itself—zeroing out critical image regions leads to unnatural artifacts that the VAE, not trained on such distributions, cannot realistically reconstruct. This severely limits its practical value, particularly for scenes requiring subtle visual coherence.

Object Detection-based Masking performed the worst overall. The main bottleneck was the failure of YOLOv5 to consistently detect relevant objects in the CARLA dataset, leading to unreliable or incomplete masking. As a result, this method frequently failed to generate valid counterfactuals and was excluded from certain evaluations due to insufficient sample coverage.

From a minimality perspective, LIME on Latent (Median) remains the most sparse technique but falls short in altering the classifier’s prediction. LIME on Image, by contrast, modifies the entire image with excessive masking, compromising realism. Grid-Based and LIME on Latent NUN methods demonstrate the most promising trade-offs, achieving a favorable balance between sparsity, proximity, validity, and execution time. These findings reinforce that counterfactual explanations must not only flip the model's prediction but do so through changes that are visually plausible, semantically meaningful, and computationally viable.

Taken together, the results strongly advocate for latent-space based masking techniques particularly LIME on Latent using NUN as the most interpretable and effective strategy for counterfactual generation in autonomous driving settings as shown in this thesis. While further optimization is required to reduce its computational overhead, this approach holds considerable promise for enhancing transparency and trust in real-world decision-making systems.





\section{Human-Centered Evaluation: Methodology and Results (RQ4)} \label{sec:human_evaluation}

While traditional quantitative metrics such as SSIM and PSNR offer insight into the visual quality of reconstructed counterfactual images, they do not fully reflect human judgment regarding interpretability, plausibility, or realism. As noted by Delaney et al.~\cite{DELANEY2023103995}, counterfactual explanations are meant to fulfill human explanation goals and must therefore be evaluated with human-centered metrics. This is especially crucial in high-stakes domains such as autonomous driving, where explanations must not only be technically valid but also intuitively understandable and semantically coherent from a user's perspective.

To incorporate this human-centered evaluation, we developed a web-based application using FastAPI for backend processing and Jinja2 for rendering the frontend. The system was designed to present users with the original input image, its predicted label, and four corresponding counterfactual explanations each generated using a different masking technique: Grid-Based Masking, LIME on Images, LIME on Latent Features, and LIME on Latent Features using the NUN method. To ensure fairness and avoid bias, the interface randomized the order of counterfactuals and concealed the identity of the underlying methods. Participants were only shown the prediction labels associated with each counterfactual image, not the technique used to generate it. A screenshot of the evaluation interface is provided in Appendix~\ref{appendix:webinterface}, Figure~\ref{fig:app:form_ui}.

Object detection-based masking was excluded from this user study due to its low counterfactual success rate. It failed to consistently produce valid counterfactual examples and thus lacked sufficient representative samples for human evaluation.

Each user was asked to evaluate the counterfactual explanations based on three criteria: interpretability (how clearly the image highlights the minimal change required to alter the model’s prediction), plausibility (how realistic and contextually appropriate the modified image appears), and visual coherence (whether the transformation affects only necessary regions while preserving the rest of the scene). These aspects were rated on a Likert scale of 1 to 5.\footnote{The Likert scale is a common psychometric scale used in questionnaires, where 1 typically represents "strongly disagree" or "poor" and 5 represents "strongly agree" or "excellent".} Users also had the option to provide qualitative comments to elaborate on their assessments. The samples shown were those where all four methods successfully generated counterfactuals (see Section~\ref{subsubsec:masking_method_overlap} for the selection criteria).

\subsection{Quantitative Analysis of User Ratings} \label{subsubsec:quantitative_analysis_of_user_ratings}

To address RQ4 (Which counterfactual explanation method is preferred by users when selecting among generated explanations of the same original image, and what factors influence user preference?), we conducted an in-depth analysis of the collected human ratings. The anonymized evaluation labels were internally mapped to the respective methods: Counterfactual\_1 corresponds to Grid-Based Masking, Counterfactual\_2 to LIME on Image Masking, Counterfactual\_3 to LIME on Latent Features, and Counterfactual\_4 to LIME on Latent Features using the NUN method.

The results of the user study, visualized in Figure~\ref{fig:bar_plot_user_eval}, show that Grid-Based Masking (Method 1) achieved the highest scores in both interpretability (4.32) and visual coherence (4.16), indicating its strength in generating localized, structured changes while preserving background consistency. In terms of plausibility, LIME on Latent Features (Method 3) and LIME on Latent Features using NUN (Method 4) performed best, scoring 4.04 and 3.96 respectively—reflecting their ability to generate semantically plausible modifications in the latent space. In contrast, LIME on Image Masking (Method 2) consistently scored the lowest across all criteria (1.00), with participants noting issues such as severe artifacts, unrealistic reconstructions, and lack of clarity.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.75\textwidth]{img/human_rating_results/bar_plot_user_evaluations.png}
    \caption[Bar plot of average user ratings for CE methods]{%
Average user ratings (scale 1–5) for each counterfactual explanation method across all evaluation criteria. Higher values indicate better perceived interpretability, plausibility, or coherence.}
    \label{fig:bar_plot_user_eval}
\end{figure}


To provide a consolidated view, we generated a heatmap (Figure~\ref{fig:heatmap_user_eval}) summarizing the average user ratings per method for each evaluation criterion. This visual representation clearly demonstrates the complementary strengths of different methods. Grid-Based Masking excels in interpretability and coherence, while latent space methods, particularly Method 3 and Method 4, produce more realistic and semantically aligned modifications.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.75\textwidth]{img/human_rating_results/heatmap_user_evaluations.png}
    \caption[Heatmap of user evaluations by method and criterion]{%
Heatmap of average user ratings (scale 1–5) for each counterfactual method across individual evaluation dimensions: interpretability, plausibility, and visual coherence. Darker shades represent stronger user agreement.}
    \label{fig:heatmap_user_eval}
\end{figure}


\vspace{0.5em}
\paragraph{Per-Criterion Analysis.}
Figures~\ref{fig:cf_interpretability}--\ref{fig:cf_visualcoherence} provide detailed views of average ratings per evaluation criterion:

\begin{itemize}
    \item Interpretability: Grid-Based Masking (Method 1) received the highest rating (4.32), followed by LIME on Latent NUN (2.64) and LIME on Latent (2.44). LIME on Image (1.00) was rated lowest due to aggressive, uninformative masking patterns.
    \item Plausibility: LIME on Latent (Method 3) was rated most plausible (4.04), with NUN-enhanced latent manipulation (Method 4) close behind (3.96). Grid-Based Masking scored moderately (2.28), while LIME on Image again received the lowest rating (1.00).
    \item Visual Coherence: Grid-Based Masking again led with 4.16, confirming its strength in preserving background information. Method 4 followed with 3.04, Method 3 scored 2.76, and Method 2 trailed at 1.00.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{img/human_rating_results/Interpretability_ratings.png}
    \caption[User-rated interpretability scores by method]{%
Average user ratings for interpretability across counterfactual explanation methods (scale 1–5). Higher scores indicate better user understanding of why the prediction changed.}
    \label{fig:cf_interpretability}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{img/human_rating_results/Plausibility_ratings.png}
    \caption[User-rated plausibility scores by method]{%
Average user ratings for plausibility across counterfactual explanation methods (scale 1–5). Ratings reflect how realistic the generated counterfactual images appear to human observers.}
    \label{fig:cf_plausibility}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{img/human_rating_results/VisualCoherence_ratings.png}
    \caption[User-rated visual coherence scores by method]{%
Average user ratings for visual coherence across counterfactual explanation methods (scale 1–5). This criterion reflects image clarity, absence of artifacts, and semantic consistency with the original.}
    \label{fig:cf_visualcoherence}
\end{figure}

\vspace{0.5em}
\subsection{Qualitative Feedback and Discussion}

In addition to numerical ratings, users provided open-ended feedback to elaborate on their experience evaluating the counterfactual explanations. A thematic analysis of these comments revealed distinct strengths and weaknesses for each method.

Grid-Based Masking (Method 1) was frequently praised for its clarity and precision. Participants described it using phrases such as "clear and coherent masking", "very clear and detailed", and "excellent clarity and structure". These remarks align with its high interpretability and visual coherence scores. Feedback such as "satisfactory masking overall" and "consistent performance" further emphasize its reliability.

LIME on Image Masking (Method 2) received universally negative feedback. Users described outputs as "image mostly black", "completely dark image", "no visible detail", and "output unusable". These comments align with its lowest ratings across all evaluation dimensions, indicating that the zero-masking strategy significantly degraded image quality and interpretability.

LIME on Latent Features (Method 3) received mixed responses. Some users described the reconstructions as "moderate quality" and "acceptable", while others noted "average quality with slight artifacts" or "over-modified regions". These qualitative impressions reflect the middle-range scores of this method.

LIME on Latent Features using NUN (Method 4) was generally well-received. Comments included "realistic modification", "more details preserved", and "mimics original scene well". These remarks highlight its effectiveness in generating semantically aligned counterfactuals. However, a few responses indicated that further refinement could improve local consistency.

Overall, qualitative feedback closely mirrored the quantitative trends. Grid-Based Masking was preferred for clarity and structure, while latent space methods especially with NUN guidance—offered realism and plausibility. LIME on Image consistently underperformed. This underscores the value of integrating user feedback in evaluating counterfactual methods, especially in critical applications like autonomous driving.