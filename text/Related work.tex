The increasing use of machine learning in high-stakes decision-making domains—such as finance, healthcare, and autonomous systems—has amplified the demand for transparency, interpretability, and user trust. In response, a wide range of post-hoc explanation techniques have been proposed to make black-box models more understandable. Model-agnostic approaches such as LIME~\cite{Ribeiro2018} and SHAP~\cite{lundberg2017unifiedapproachinterpretingmodel} explain individual predictions by fitting simpler surrogate models to approximate a model’s local decision boundary. Similarly, gradient based methods such as Integrated Gradients~\cite{8237336} and Influence Functions~\cite{pmlr-v70-koh17a} analyze how small input changes affect a model’s output. While these techniques help identify important features, they often fall short in supporting causal reasoning or answering "what-if" questions an essential aspect of human-centric explanation.


To address these limitations, recent research has shifted toward generating counterfactuals in latent space using deep generative models such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs). A notable milestone in this evolution is the Contrastive Explanation Method (CEM) proposed by Dhurandhar et al.~\cite{DBLP:journals/corr/abs-1802-07623}, which introduced the concept of pertinent positives (PP) features that must be present and pertinent negatives (PN)—features that must be absent for a classification to hold. By employing a convolutional autoencoder (CAE), CEM ensures explanations remain close to the data manifold. While conceptually aligned with human reasoning, its use of a CAE and reliance on simple datasets like MNIST limit its scalability and generalizability.

Our work builds upon the philosophical grounding of CEM by leveraging VAEs, which provide a structured and probabilistic latent space for more expressive and scalable counterfactual generation. We further introduce semantic masking strategies, including grid-based and object-aware masking, that selectively modify latent regions to produce interpretable and class flipping counterfactuals. Pawelczyk et al.~\cite{Pawelczyk_2020} contribute to this direction by proposing density-aware counterfactuals, which improve plausibility through latent optimization that respects the data distribution.

While CEILS~\cite{crupi2021counterfactualexplanationsinterventionslatent} performs latent space interventions grounded in structural causal models (SCMs) primarily for tabular data, our method operates in a generative setting for image based counterfactuals and also on tabular data. Both approaches share the goal of generating semantically valid explanations via latent manipulation, but our framework is tailored toward visual interpretability and defines counterfactuals by classifier decision boundary crossings. Similarly, our work shares conceptual alignment with SharpShooter~\cite{barr2021counterfactualexplanationslatentspace}, which generates counterfactuals by interpolating between latent representations of source and target classes using a dual VAE setup. Their method uses classifier flip as a decision criterion and emphasizes plausibility via reconstruction loss and classifier shift. In contrast, we introduce targeted semantic interventions rather than linear interpolation. These interventions include masking latent variables related to specific objects (via object detection), spatial regions (via grid-based masking), or important features in the latent space (via LIME-based attribution). 

Wijekoon et al.~\cite{WijekoonWNMPC21} introduced a strategy for counterfactual generation in student outcome prediction using the concept of Nearest Unlike Neighbor (NUN) in Euclidean space. Their framework focused on finding the most similar instance from a different class and then using LIME to rank features by their influence on the classification outcome. Two actionable feature ordering strategies were proposed: (1) modifying query features based on their own LIME importance, and (2) modifying query features based on the LIME importance of the NUN the latter being shown to produce more compact and efficient counterfactuals. Inspired by this approach, we extend their methodology to the latent space of deep generative models, where we first locate the NUN in latent space and apply LIME to identify the most influential latent features. We iteratively replace the latent features of the query with those from the NUN, guided by their LIME importance, until the classifier prediction changes. This allows us to generate visually plausible and semantically meaningful counterfactual images with minimal changes in the latent representation.

Counterfactual explanations have been widely applied across domains. In finance, they are used to explain loan approvals, credit scoring, and risk predictions by suggesting actionable changes such as income or debt level adjustments~\cite{guidotti2022counterfactual, DELANEY2023103995, Rudin2019}. In healthcare, counterfactuals assist in understanding which symptom changes could alter a diagnosis~\cite{10.1145/3351095.3372855}. In employment and education, they provide insight into hiring or admissions decisions or student performance in studies~\cite{WijekoonWNMPC21}. In manufacturing, they help interpret quality control outcomes~\cite{icpram24}, and in autonomous driving, object-aware counterfactuals clarify scene-level misclassifications and perception errors~\cite{zemni2023octetobjectawarecounterfactualexplanations}.

Despite significant progress, challenges remain. Many approaches are computationally expensive, require access to model gradients, or generate explanations that lack visual interpretability. Methods that operate directly in input space often yield unnatural outputs, while latent space techniques may trade off transparency for plausibility.  Moreover, most methods focus on local, instance specific explanations, offering limited insight into broader model behavior or user understandable semantics.

To address these limitations, this thesis introduces a structured counterfactual generation framework based on latent-space manipulation using a Variational Autoencoder (VAE). By selectively masking latent components associated with spatial regions or detected objects, the proposed approach enables the creation of realistic, class-altering, and semantically meaningful counterfactual images. This bridges the gap between interpretability and plausibility and makes the method suitable for human-centered, vision-based applications.

While most counterfactual literature relies heavily on quantitative evaluation—such as validity, proximity, sparsity, and plausibility the need for human centered evaluation has gained increasing attention. Delaney et al.\cite{DELANEY2023103995} introduced a novel framework in which human participants edited misclassified images to create their own counterfactuals. Their findings revealed that humans tend to favor semantically coherent and prototype aligned changes, contrasting with machine generated counterfactuals that emphasize minimal edits. These insights emphasize that effective explanations must not only satisfy mathematical constraints but also align with human expectations. Motivated by this, we conduct a human evaluation study in which participants assess the visual interpretability, plausibility, and coherence of our VAE based counterfactuals. This human-centered perspective complements traditional metrics and provides a more holistic view of explanation quality, as detailed in Section\ref{section:Human Evaluation of Counterfactual Explanations}.


Variational Autoencoders (VAEs) have emerged as one of the most influential generative modeling frameworks, combining the representational power of deep neural networks with the principles of probabilistic inference. Introduced by Kingma and Welling~\cite{Kingma_2019}, the VAE architecture formulates generative modeling as the optimization of the Evidence Lower Bound (ELBO), which balances reconstruction fidelity with latent space regularization through Kullback-Leibler (KL) divergence. The introduction of the reparameterization trick was a key innovation, enabling efficient gradient-based optimization of stochastic latent variables. In this thesis, the VAE framework provides the foundation for learning meaningful latent representations from image data. Core principles such as the ELBO objective, stochastic sampling, and the interplay between reconstruction loss and latent regularization are implemented and extended to support counterfactual generation in a structured, interpretable latent space.

Subsequent advancements in VAE research have refined its generative capabilities and enhanced robustness. Chen et al.~\cite{chen2019log} proposed replacing the traditional mean squared error (MSE) reconstruction loss with a log-cosh loss, which behaves like L2 loss for small residuals and transitions to L1 behavior for larger errors. This hybrid formulation improves robustness to outliers while preserving fine image details, making it particularly effective for visual tasks. In this thesis, we adopt the log-cosh loss during VAE training and compare its impact on image quality and latent structure against conventional MSE-based reconstruction see the~\cref{reconstruction_loss}.

The flexibility of VAEs has led to their adoption across a wide range of applications, from synthetic data generation and image translation to clustering and unsupervised anomaly detection. For instance, Chen et al.~\cite{chen2019log} demonstrated the versatility of VAEs by applying them to creative domains such as game level generation and anime avatar synthesis. These examples underscore the broad applicability and expressive power of VAEs in both structured and unstructured domains.

Furthermore, several researchers integrated counterfactual explanations within the VAE framework to improve interpretability and usability in practical scenarios. Ernst~\cite{ernst2024counterfactual} specifically studied integrating counterfactual explanations within the VAE architecture for anomaly detection tasks in tabular data. His approach involved modifications to the standard VAE objective to facilitate the generation of meaningful counterfactual instances, directly aligning with our thesis, which leverages counterfactual explanations in the latent spaces learned by VAEs.

These probabilistic modeling properties position VAEs as a powerful foundation for interpretable, visually coherent counterfactual generation. In contrast to traditional autoencoders, VAEs promote smooth, continuous latent spaces where neighboring points correspond to semantically similar reconstructions. This makes them particularly well-suited for image based tasks where realism and interpretability are critical, such as those encountered in safety critical applications like autonomous driving.

Parallel to advances in generative modeling, the development of Local Interpretable Model-Agnostic Explanations (LIME)~\cite{Ribeiro2018} has provided a framework for post-hoc interpretability of any black-box model. LIME explains a specific prediction by approximating the local decision boundary with an interpretable surrogate model typically a sparse linear regression—fitted to perturbed samples around the original input. While LIME does not natively produce counterfactuals, it provides feature attribution scores that highlight influential input dimensions. These attributions have been extended or adapted in several works for use in counterfactual generation, particularly for identifying which features to mask or alter. In this thesis, we integrate LIME-based importance scores within the latent space of a VAE to guide feature level masking, enabling targeted interventions that flip classifier predictions while preserving visual plausibility.

Together, VAEs and LIME form the technical backbone of our framework: the VAE provides a generative latent space for semantically meaningful edits, while LIME identifies high-impact features to prioritize during counterfactual generation. This combination supports the creation of interpretable, realistic, and class-altering explanations for complex image classification tasks.

Agarwal and Nguyen~\cite{agarwal2020explainingimageclassifiersremoving} proposed a generative approach to improve the realism of perturbation based explainability methods. They combined standard attribution techniques such as LIME, Saliency, and Meaningful Perturbations with pretrained inpainting networks (e.g., DeepFill v1/v2) to realistically fill masked regions in input images. Their work demonstrated that traditional masking methods often lead to unnatural perturbations, potentially misleading explanation results, and that generative models can restore semantic consistency in masked inputs. They evaluated their approach using object localization, deletion scores, and saliency metrics on benchmark datasets such as ImageNet and Places365. Inspired by their motivation to ensure realistic masked inputs, we employ a domain-specific Variational Autoencoder (VAE) to reconstruct and re-encode masked images within our counterfactual generation pipeline elaborated in \cref{sec:lime_on_images}.







