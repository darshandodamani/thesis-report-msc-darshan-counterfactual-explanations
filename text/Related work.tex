This chapter provides a comprehensive review of prior research relevant to this thesis, structured across multiple areas including explainability methods, counterfactual explanations, generative modeling approaches, semantic and spatial interventions in latent/image space, and applications in safety-critical domains. The review ends with a comparative discussion positioning this thesis in the context of existing work.



\section{Explainability in Machine Learning}
Numerous post-hoc interpretability techniques have been proposed to make complex black-box models more interpretable. Model-agnostic methods are the most widely used among them, such as LIME~\cite{Ribeiro2018} and SHAP~\cite{lundberg2017unifiedapproachinterpretingmodel}. Such techniques explain specific predictions by fitting simpler surrogate models locally around a given input. SHAP is based on cooperative game theory principles, injecting a certain amount of rigor into the process of attributing the contributions of features to the outputs while guaranteeing consistency and local accuracy.

Gradient-based approaches such as Integrated Gradients~\cite{8237336} and Influence Functions~\cite{pmlr-v70-koh17a} use model derivatives to attribute importance to input features. These methods have been widely applied in image classification, text classification, and tabular data settings. While these techniques offer insights into feature attribution, they primarily identify important features rather than provide actionable guidance on how to change a prediction.

While these methods help understand which features are important, my thesis goes further by offering actionable changes through counterfactual explanations. Unlike LIME and SHAP which operate in the input or interpretable feature space, my method operates in the latent space learned by a VAE, yielding explanations that are more compact, realistic, and class-altering.


\section{Counterfactual Explanations}

Counterfactual explanations aim to identify the minimal changes required to an input that would lead to a different model prediction~\cite{wachter2018CE}. This paradigm offers an intuitive and human-centered method for explaining machine learning decisions, particularly in high-stakes contexts such as finance, healthcare, and autonomous systems. For example, a counterfactual explanation might suggest: *“If your income were \$5,000 higher, the loan would have been approved.”* Such explanations are valued for their simplicity and directness, guiding users toward actionable changes without overwhelming them with implausible modifications. Wachter et al.~\cite{wachter2018CE} were the first to formalize this concept in a general optimization-based framework.

\subsection{Optimization-Based Counterfactuals}

Wachter et al.~\cite{wachter2018CE} proposed a method that balances proximity to the original input with the requirement to cross the model’s decision boundary. However, their approach operates in the raw feature space and struggles to scale to high-dimensional, unstructured data such as images due to challenges in visual coherence and realism. This work builds upon Wachter’s principle of minimal, class-altering modifications but applies it to unstructured image domains using latent representations learned by a Variational Autoencoder (VAE). The proposed method preserves the model-agnostic and post-hoc nature of counterfactual generation while ensuring that modifications are visually plausible, semantically meaningful, and classifier-relevant.

Pawelczyk et al.~\cite{Pawelczyk_2020} introduced density-aware counterfactuals that preserve plausibility by optimizing solutions to remain near the data manifold. Similarly, this work decodes counterfactual images directly from the VAE latent space, ensuring that generated samples lie on the learned manifold of realistic driving scenes. This allows counterfactuals to maintain high visual fidelity while altering only the latent dimensions critical for changing predictions.

\subsection{Causality-Aware Counterfactuals}

CEILS~\cite{crupi2021counterfactualexplanationsinterventionslatent} (Counterfactual Explanations as Interventions in Latent Space) extends counterfactual generation into the latent space by integrating structural causal models (SCMs). Their approach enables interventions that respect causal dependencies, such as changing education leading to adjustments in income. While the proposed method does not incorporate explicit SCMs, it introduces structure and interpretability through attribution-guided latent masking and Nearest Unlike Neighbor (NUN)-based replacement. These techniques ensure that only causally relevant and class-distinguishing components of the latent representation are modified, supporting the goal of coherent and targeted counterfactuals in image space.

Karimi et al.~\cite{karimi2020algorithmic} emphasized that counterfactuals must be generated with causal awareness, warning against spurious or purely correlational interventions. Although this thesis does not construct formal causal graphs, it aligns with their motivation by modifying features that are attributionally causal i.e., identified by LIME or NUN as contributing significantly to a prediction while preserving the scene’s semantic structure through latent reconstructions.

\subsection{Diversity and Actionability}

Ustun et al.~\cite{ustun2019actionable} introduced the concept of actionable recourse\footnote{Actionable Recourse refers to the ability of an individual to alter a machine learning model's prediction by making feasible changes to specific, modifiable input variables.} by incorporating domain-specific constraints, disallowing changes to immutable or infeasible features such as age or race. Although the proposed method does not explicitly encode such constraints, it ensures actionability in image contexts through semantic object masking and attribution-based feature prioritization. For example, altering a road sign or saliency-highlighted region is a semantically interpretable and plausible modification that aligns with how humans understand scene-based classification.

Mothilal et al.~\cite{DBLP:journals/corr/abs-1905-07697} addressed the need for diverse recourse options by proposing a framework to generate multiple counterfactuals that span different plausible paths toward a desired prediction. While this method does not explicitly optimize for diversity via sampling, it achieves functional diversity through the use of multiple masking strategies (grid-based, object-based, and latent-feature-based), each producing distinct and interpretable changes to the input scene.

\subsection{Counterfactuals in Unstructured Domains}

Extending counterfactual explanations to unstructured data such as images introduces challenges in perceptual realism, semantic consistency, and computational tractability. Techniques such as CEM~\cite{DBLP:journals/corr/abs-1802-07623}, SharpShooter~\cite{barr2021counterfactualexplanationslatentspace}, and OCTET~\cite{zemni2023octetobjectawarecounterfactualexplanations} (Object-aware Counterfactual Explanations) address these challenges via contrastive latent editing, interpolation, or compositional generative modeling. SharpShooter generates counterfactuals by interpolating between latent encodings of different classes to induce classifier prediction changes. OCTET modifies scene elements using external generators. This work draws inspiration from these methods but introduces a unified framework that integrates VAE-based reconstructions, semantic object and grid masking, and attribution-aware latent interventions, enabling interpretable, class-flipping, and visually coherent counterfactuals in safety-critical driving environments.

\begin{sidewaystable}[p]
\centering
\begin{tabular}{|l|c|c|c|c|c|p{3cm}|}
\hline
\textbf{Method} & \textbf{Domain} & \textbf{Causal?} & \textbf{Actionable?} & \textbf{Diverse?} & \textbf{Unstructured\footnote{Unstructured refers to high-dimensional data types such as images or natural language}} & \textbf{Comparison to This Work} \\ \hline
Wachter et al. & Tabular\footnote{Tabular refers to structured input features such as income, age, or education.} & \xmark & \xmark & \xmark & \xmark & Extended to image domain via VAE \\ \hline
CEILS & Tabular & \cmark & \cmark & \xmark & \xmark & Latent masking for causal relevance \\ \hline
Mothilal et al. & Tabular & \xmark & \cmark & \cmark & \xmark & Functional diversity via masking \\ \hline
OCTET & Vision & \xmark & \xmark & \xmark & \cmark & Uses VAE masking instead of external generator \\ \hline
\textbf{This Work} & Vision + Tabular\footnote{Vision + Tabular indicates that this work uses images both in pixel space (e.g., masking) and in structured latent representations akin to tabular data.} & $\sim$ (Attrib) & \cmark & $\sim$ (Multi-path) & \cmark & Unified, interpretable, realistic explanation framework combining semantic, spatial, and latent-level masking. \\ \hline
\end{tabular}
\caption{Comparison of counterfactual explanation methods across dimensions.}
\label{tab:ce_comparison}
\vspace{1em}
\noindent
\end{sidewaystable}




\section{Generative Models for Counterfactual Explanations}

To address the challenge of generating counterfactuals for high-dimensional inputs such as images, several studies have adopted generative models like Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs).

VAEs~\cite{Kingma_2019} encode data into a structured latent space that facilitates controlled generation of new samples. Enhancements like log-cosh loss~\cite{chen2019log} improve reconstruction robustness. Ernst~\cite{ernst2024counterfactual} modifies the VAE objective for anomaly detection and interpretable counterfactuals in tabular domains.

The Contrastive Explanation Method (CEM)~\cite{DBLP:journals/corr/abs-1802-07623} introduces pertinent positives and negatives using a convolutional autoencoder to generate contrastive explanations. SharpShooter~\cite{barr2021counterfactualexplanationslatentspace} interpolates between latent encodings of different classes to induce classifier prediction changes. GAN-based techniques, such as Residual GANs~\cite{nemirovsky2021countergangeneratingrealisticcounterfactuals}, add adversarial perturbations to the latent space for generating class-altering counterfactuals.

My work aligns with CEM in using autoencoders for realism but differs in that I do not optimize for perturbations directly. Instead, I use masking strategies to drive prediction changes this avoids adversarial artifacts while maintaining plausibility. Compared to interpolation based SharpShooter, my method ensures changes are sparse and guided by interpretability methods (e.g., LIME), allowing for more interpretable decision paths.


\section{Semantic and Spatial Masking Techniques for Visual Counterfactuals}
Another approach to generating interpretable visual counterfactuals involves modifying semantically meaningful image regions. This includes masking detected objects or spatially-defined regions, then reconstructing them to observe changes in classification.

OCTET~\cite{zemni2023octetobjectawarecounterfactualexplanations} modifies objects in autonomous driving scenes to flip predictions, preserving semantic coherence and realism. Grid-based and saliency-based masking provide localized focus, though may lack semantic granularity. Agarwal and Nguyen~\cite{agarwal2020explainingimageclassifiersremoving} propose combining attribution-based masks with pretrained inpainting models like DeepFill, improving visual plausibility and reducing interpretive artifacts.

OCTET modifies scene elements using external generators. My approach, in contrast, uses a single trained VAE for both reconstruction and explanation. Unlike methods requiring pretrained inpainting models, my grid-based, LIME on Image masking and object-detectionbased masking methods achieve realism using only the data manifold captured during VAE training. This makes my method lighter and more flexible, particularly for synthetic datasets like CARLA.

\section{Attribution-Guided Counterfactual Generation}
Recent works integrate attribution methods into the counterfactual generation process to identify impactful features for alteration.

Wijekoon et al.~\cite{WijekoonWNMPC21} propose the Nearest Unlike Neighbor (NUN) strategy, using LIME to rank features and sequentially replacing them based on importance. This results in sparse and interpretable counterfactuals. DisCERN~\cite{wiratunga2021discerndiscoveringcounterfactualexplanationsusing} uses feature relevance from LIME or SHAP to guide counterfactual edits, avoiding optimization-based adaptation. A variant applies Integrated Gradients with class-specific baselines, reducing feature modifications and improving explanation stability. Hybrid methods that combine attribution and latent space editing (e.g., with VAEs or GANs) offer greater semantic control and visual consistency, particularly in high-dimensional image domains.

While NUN use LIME/SHAP to guide interventions in tabular settings, my thesis introduces LIME-guided NUN masking directly in the VAE latent space a novel extension for high-dimensional images. This approach combines semantic precision (via LIME) with realism (via latent manipulation), achieving high validity and interpretability in my user studies, outperforming traditional LIME on image approaches.

\section{Applications of Counterfactual Explanations}
Counterfactual explanations have been applied in various domains where interpretability and fairness are critical. In finance, they aid in loan decisions and credit scoring~\cite{guidotti2022counterfactual, DELANEY2023103995}. In healthcare, they highlight changes in clinical features affecting diagnoses~\cite{10.1145/3351095.3372855}. In education and employment, they explain student and hiring outcomes~\cite{WijekoonWNMPC21}.

In autonomous driving, counterfactuals help debug perception modules. Zemni et al.\cite{zemni2023octetobjectawarecounterfactualexplanations} demonstrate modifying scene elements to correct misclassifications. Rudin\cite{Rudin2019} emphasizes the use of inherently interpretable models in life-critical applications, reinforcing the importance of reliable visual explanations.

My work focuses on autonomous driving a relatively underexplored yet critical application area for counterfactual explanations. Unlike prior work limited to object-level interventions (e.g., OCTET), my thesis investigates both semantic object masking and latent-level perturbations, enabling complex model exploration of how hidden visual features influence navigation decisions.

\section{Summary and Positioning of This Work}
This thesis builds on prior research in generative modeling, attribution-guided interventions, and semantic masking to develop a VAE-based counterfactual explanation framework tailored for autonomous driving.

Unlike interpolation-based or adversarial techniques, the implemented approach in this thesis focuses on semantically grounded masking strategies including image based, and latent feature masking to flip classifier predictions while preserving realism. LIME-based attribution and Nearest Unlike Neighbor (NUN) guidance are integrated in the latent space to enhance sparsity and interpretability.

Evaluation includes both quantitative metrics and a user study, aligning with recent research emphasizing human-centered assessment~\cite{DELANEY2023103995}. This positions the work within the broader goal of building trustworthy, interpretable AI systems for safety-critical applications.