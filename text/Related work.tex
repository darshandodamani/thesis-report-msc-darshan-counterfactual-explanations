This chapter provides a comprehensive review of prior research relevant to this thesis, structured across multiple areas including explainability methods, counterfactual explanations, generative modeling approaches, semantic and spatial interventions in latent/image space, and applications in safety-critical domains. The review ends with a comparative discussion positioning this thesis in the context of existing work.



\section{Explainability in Machine Learning}
Numerous post-hoc interpretability techniques have been proposed to make complex black-box models more interpretable. Model-agnostic methods are the most widely used among them, such as LIME~\cite{Ribeiro2018} and SHAP~\cite{lundberg2017unifiedapproachinterpretingmodel}. Such techniques explain specific predictions by fitting simpler surrogate models locally around a given input. SHAP is based on cooperative game theory principles, injecting a certain amount of rigor into the process of attributing the contributions of features to the outputs while guaranteeing consistency and local accuracy.

Gradient-based approaches such as Integrated Gradients~\cite{8237336} and Influence Functions~\cite{pmlr-v70-koh17a} use model derivatives to attribute importance to input features. These methods have been widely applied in image classification, text classification, and tabular data settings. While these techniques offer insights into feature attribution, they primarily identify important features rather than provide actionable guidance on how to change a prediction.


\section{Counterfactual Explanations}
Counterfactual explanations identify minimal changes to input features that would alter a model's prediction~\cite{wachter2018CE}. They are valuable in high-stakes settings where understanding model behavior and suggesting actionable changes are essential.

In structured data domains, CEILS~\cite{crupi2021counterfactualexplanationsinterventionslatent} performs latent space interventions using structural causal models. Pawelczyk et al.\cite{Pawelczyk_2020} propose density-aware counterfactuals that maintain plausibility by optimizing within the data manifold. Ustun et al.\cite{ustun2019actionable} focus on actionable recourse, while Mothilal et al.\cite{DBLP:journals/corr/abs-1905-07697} introduce diversity to generate multiple plausible counterfactuals. Karimi et al.\cite{karimi2020algorithmic} emphasize the integration of causal assumptions.

Recent optimization-based methods balance constraints such as proximity, sparsity, plausibility, and diversity~\cite{NEURIPS2021_fd0a5a5e, delser2022tradeoff}, but their extension to unstructured data like images is non-trivial due to dimensionality and perceptual realism requirements.




\section{Generative Models for Counterfactual Explanations}

To address the challenge of generating counterfactuals for high-dimensional inputs such as images, several studies have adopted generative models like Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs).

VAEs~\cite{Kingma_2019} encode data into a structured latent space that facilitates controlled generation of new samples. Enhancements like log-cosh loss~\cite{chen2019log} improve reconstruction robustness. Ernst~\cite{ernst2024counterfactual} modifies the VAE objective for anomaly detection and interpretable counterfactuals in tabular domains.

The Contrastive Explanation Method (CEM)~\cite{DBLP:journals/corr/abs-1802-07623} introduces pertinent positives and negatives using a convolutional autoencoder to generate contrastive explanations. SharpShooter~\cite{barr2021counterfactualexplanationslatentspace} interpolates between latent encodings of different classes to induce classifier prediction changes. GAN-based techniques, such as Residual GANs~\cite{nemirovsky2021countergangeneratingrealisticcounterfactuals}, add adversarial perturbations to the latent space for generating class-altering counterfactuals.


\section{Semantic and Spatial Masking Techniques for Visual Counterfactuals}
Another approach to generating interpretable visual counterfactuals involves modifying semantically meaningful image regions. This includes masking detected objects or spatially-defined regions, then reconstructing them to observe changes in classification.

OCTET~\cite{zemni2023octetobjectawarecounterfactualexplanations} modifies objects in autonomous driving scenes to flip predictions, preserving semantic coherence and realism. Grid-based and saliency-based masking provide localized focus, though may lack semantic granularity.

Agarwal and Nguyen~\cite{agarwal2020explainingimageclassifiersremoving} propose combining attribution-based masks with pretrained inpainting models like DeepFill, improving visual plausibility and reducing interpretive artifacts.

\section{Attribution-Guided Counterfactual Generation}
Recent works integrate attribution methods into the counterfactual generation process to identify impactful features for alteration.

Wijekoon et al.~\cite{WijekoonWNMPC21} propose the Nearest Unlike Neighbor (NUN) strategy, using LIME to rank features and sequentially replacing them based on importance. This results in sparse and interpretable counterfactuals.

DisCERN~\cite{wiratunga2021discerndiscoveringcounterfactualexplanationsusing} uses feature relevance from LIME or SHAP to guide counterfactual edits, avoiding optimization-based adaptation. A variant applies Integrated Gradients with class-specific baselines, reducing feature modifications and improving explanation stability.

Hybrid methods that combine attribution and latent space editing (e.g., with VAEs or GANs) offer greater semantic control and visual consistency, particularly in high-dimensional image domains.

\section{Applications of Counterfactual Explanations}
Counterfactual explanations have been applied in various domains where interpretability and fairness are critical. In finance, they aid in loan decisions and credit scoring~\cite{guidotti2022counterfactual, DELANEY2023103995}. In healthcare, they highlight changes in clinical features affecting diagnoses~\cite{10.1145/3351095.3372855}. In education and employment, they explain student and hiring outcomes~\cite{WijekoonWNMPC21}.

In autonomous driving, counterfactuals help debug perception modules. Zemni et al.\cite{zemni2023octetobjectawarecounterfactualexplanations} demonstrate modifying scene elements to correct misclassifications. Rudin\cite{Rudin2019} emphasizes the use of inherently interpretable models in life-critical applications, reinforcing the importance of reliable visual explanations.

\section{Summary and Positioning of This Work}
This thesis builds on prior research in generative modeling, attribution-guided interventions, and semantic masking to develop a VAE-based counterfactual explanation framework tailored for autonomous driving.

Unlike interpolation-based or adversarial techniques, the implemented approach in this thesis focuses on semantically grounded masking strategies including image based, and latent feature masking to flip classifier predictions while preserving realism. LIME-based attribution and Nearest Unlike Neighbor (NUN) guidance are integrated in the latent space to enhance sparsity and interpretability.

Evaluation includes both quantitative metrics and a user study, aligning with recent research emphasizing human-centered assessment~\cite{DELANEY2023103995}. This positions the work within the broader goal of building trustworthy, interpretable AI systems for safety-critical applications.